2025-05-21 14:41:04,136 - root - INFO - Log file is ../log/mnist-fashionmnist/log.txt.
2025-05-21 14:41:04,137 - root - INFO - Data path is ../data.
2025-05-21 14:41:04,137 - root - INFO - Export path is ../log/mnist-fashionmnist.
2025-05-21 14:41:04,137 - root - INFO - Dataset: mnist-fashionmnist
2025-05-21 14:41:04,137 - root - INFO - Normal class: [0]
2025-05-21 14:41:04,137 - root - INFO - Conducting cross-dataset experiment...
2025-05-21 14:41:04,137 - root - INFO - Network: mnist_LeNet
2025-05-21 14:41:04,137 - root - INFO - Deep SVDD objective: one-class
2025-05-21 14:41:04,137 - root - INFO - Nu-paramerter: 0.10
2025-05-21 14:41:04,177 - root - INFO - Computation device: cuda
2025-05-21 14:41:04,177 - root - INFO - Number of dataloader workers: 0
2025-05-21 14:42:10,899 - root - INFO - Log file is ../log/mnist-fashionmnist/log.txt.
2025-05-21 14:42:10,899 - root - INFO - Data path is ../data.
2025-05-21 14:42:10,899 - root - INFO - Export path is ../log/mnist-fashionmnist.
2025-05-21 14:42:10,899 - root - INFO - Dataset: mnist-fashionmnist
2025-05-21 14:42:10,899 - root - INFO - Normal class: [0]
2025-05-21 14:42:10,899 - root - INFO - Conducting cross-dataset experiment...
2025-05-21 14:42:10,899 - root - INFO - Network: mnist_LeNet
2025-05-21 14:42:10,899 - root - INFO - Deep SVDD objective: one-class
2025-05-21 14:42:10,899 - root - INFO - Nu-paramerter: 0.10
2025-05-21 14:42:10,940 - root - INFO - Computation device: cuda
2025-05-21 14:42:10,940 - root - INFO - Number of dataloader workers: 0
2025-05-21 14:42:23,366 - root - INFO - Pretraining: True
2025-05-21 14:42:23,366 - root - INFO - Pretraining optimizer: adam
2025-05-21 14:42:23,366 - root - INFO - Pretraining learning rate: 0.0001
2025-05-21 14:42:23,366 - root - INFO - Pretraining epochs: 150
2025-05-21 14:42:23,366 - root - INFO - Pretraining learning rate scheduler milestones: (50,)
2025-05-21 14:42:23,366 - root - INFO - Pretraining batch size: 200
2025-05-21 14:42:23,366 - root - INFO - Pretraining weight decay: 0.0005
2025-05-21 14:42:26,672 - root - INFO - Starting pretraining...
2025-05-21 14:42:32,370 - root - INFO -   Epoch 1/150	 Time: 5.697	 Loss: 93.52280455
2025-05-21 14:42:37,639 - root - INFO -   Epoch 2/150	 Time: 5.268	 Loss: 47.46587575
2025-05-21 14:42:42,927 - root - INFO -   Epoch 3/150	 Time: 5.288	 Loss: 38.95427228
2025-05-21 14:42:48,183 - root - INFO -   Epoch 4/150	 Time: 5.255	 Loss: 32.96604865
2025-05-21 14:42:53,433 - root - INFO -   Epoch 5/150	 Time: 5.249	 Loss: 28.90617754
2025-05-21 14:42:58,703 - root - INFO -   Epoch 6/150	 Time: 5.270	 Loss: 26.05440460
2025-05-21 14:43:03,945 - root - INFO -   Epoch 7/150	 Time: 5.242	 Loss: 23.95368491
2025-05-21 14:43:09,193 - root - INFO -   Epoch 8/150	 Time: 5.248	 Loss: 22.30096610
2025-05-21 14:43:14,462 - root - INFO -   Epoch 9/150	 Time: 5.269	 Loss: 20.97895830
2025-05-21 14:43:19,720 - root - INFO -   Epoch 10/150	 Time: 5.258	 Loss: 19.88126873
2025-05-21 14:43:24,964 - root - INFO -   Epoch 11/150	 Time: 5.244	 Loss: 18.96037539
2025-05-21 14:43:30,211 - root - INFO -   Epoch 12/150	 Time: 5.247	 Loss: 18.18635558
2025-05-21 14:43:35,466 - root - INFO -   Epoch 13/150	 Time: 5.254	 Loss: 17.50044654
2025-05-21 14:43:40,745 - root - INFO -   Epoch 14/150	 Time: 5.279	 Loss: 16.91856407
2025-05-21 14:43:46,002 - root - INFO -   Epoch 15/150	 Time: 5.257	 Loss: 16.38841522
2025-05-21 14:43:51,259 - root - INFO -   Epoch 16/150	 Time: 5.256	 Loss: 15.93628148
2025-05-21 14:43:56,518 - root - INFO -   Epoch 17/150	 Time: 5.259	 Loss: 15.52938170
2025-05-21 14:44:01,785 - root - INFO -   Epoch 18/150	 Time: 5.267	 Loss: 15.16963237
2025-05-21 14:44:07,087 - root - INFO -   Epoch 19/150	 Time: 5.301	 Loss: 14.83113870
2025-05-21 14:44:12,347 - root - INFO -   Epoch 20/150	 Time: 5.260	 Loss: 14.53829748
2025-05-21 14:44:17,613 - root - INFO -   Epoch 21/150	 Time: 5.265	 Loss: 14.25767363
2025-05-21 14:44:22,873 - root - INFO -   Epoch 22/150	 Time: 5.260	 Loss: 14.00103420
2025-05-21 14:44:28,144 - root - INFO -   Epoch 23/150	 Time: 5.268	 Loss: 13.75421977
2025-05-21 14:44:33,419 - root - INFO -   Epoch 24/150	 Time: 5.275	 Loss: 13.51403233
2025-05-21 14:44:38,721 - root - INFO -   Epoch 25/150	 Time: 5.302	 Loss: 13.31350535
2025-05-21 14:44:43,972 - root - INFO -   Epoch 26/150	 Time: 5.251	 Loss: 13.10317946
2025-05-21 14:44:49,224 - root - INFO -   Epoch 27/150	 Time: 5.252	 Loss: 12.93081071
2025-05-21 14:44:54,471 - root - INFO -   Epoch 28/150	 Time: 5.247	 Loss: 12.76313965
2025-05-21 14:44:59,720 - root - INFO -   Epoch 29/150	 Time: 5.249	 Loss: 12.59547069
2025-05-21 14:45:04,984 - root - INFO -   Epoch 30/150	 Time: 5.263	 Loss: 12.44974530
2025-05-21 14:45:10,249 - root - INFO -   Epoch 31/150	 Time: 5.265	 Loss: 12.29679626
2025-05-21 14:45:15,532 - root - INFO -   Epoch 32/150	 Time: 5.282	 Loss: 12.17513930
2025-05-21 14:45:20,783 - root - INFO -   Epoch 33/150	 Time: 5.251	 Loss: 12.04091585
2025-05-21 14:45:26,040 - root - INFO -   Epoch 34/150	 Time: 5.257	 Loss: 11.93641328
2025-05-21 14:45:31,290 - root - INFO -   Epoch 35/150	 Time: 5.251	 Loss: 11.82200610
2025-05-21 14:45:36,553 - root - INFO -   Epoch 36/150	 Time: 5.262	 Loss: 11.72744310
2025-05-21 14:45:41,823 - root - INFO -   Epoch 37/150	 Time: 5.270	 Loss: 11.62787697
2025-05-21 14:45:47,093 - root - INFO -   Epoch 38/150	 Time: 5.270	 Loss: 11.54773860
2025-05-21 14:45:52,360 - root - INFO -   Epoch 39/150	 Time: 5.267	 Loss: 11.44935420
2025-05-21 14:45:57,635 - root - INFO -   Epoch 40/150	 Time: 5.274	 Loss: 11.37082374
2025-05-21 14:46:02,941 - root - INFO -   Epoch 41/150	 Time: 5.306	 Loss: 11.31141094
2025-05-21 14:46:08,217 - root - INFO -   Epoch 42/150	 Time: 5.276	 Loss: 11.22493926
2025-05-21 14:46:13,481 - root - INFO -   Epoch 43/150	 Time: 5.264	 Loss: 11.15872655
2025-05-21 14:46:18,748 - root - INFO -   Epoch 44/150	 Time: 5.266	 Loss: 11.10071121
2025-05-21 14:46:24,016 - root - INFO -   Epoch 45/150	 Time: 5.267	 Loss: 11.03230653
2025-05-21 14:46:29,282 - root - INFO -   Epoch 46/150	 Time: 5.266	 Loss: 10.97887606
2025-05-21 14:46:34,551 - root - INFO -   Epoch 47/150	 Time: 5.268	 Loss: 10.92112615
2025-05-21 14:46:39,819 - root - INFO -   Epoch 48/150	 Time: 5.268	 Loss: 10.86111217
2025-05-21 14:46:45,089 - root - INFO -   Epoch 49/150	 Time: 5.270	 Loss: 10.80113721
2025-05-21 14:46:50,371 - root - INFO -   Epoch 50/150	 Time: 5.281	 Loss: 10.70333035
2025-05-21 14:46:50,371 - root - INFO -   LR scheduler: new learning rate is 1e-05
2025-05-21 14:46:55,676 - root - INFO -   Epoch 51/150	 Time: 5.305	 Loss: 10.69958500
2025-05-21 14:47:00,932 - root - INFO -   Epoch 52/150	 Time: 5.255	 Loss: 10.69067339
2025-05-21 14:47:06,193 - root - INFO -   Epoch 53/150	 Time: 5.260	 Loss: 10.67910340
2025-05-21 14:47:11,456 - root - INFO -   Epoch 54/150	 Time: 5.263	 Loss: 10.68181378
2025-05-21 14:47:16,722 - root - INFO -   Epoch 55/150	 Time: 5.266	 Loss: 10.68060553
2025-05-21 14:47:21,982 - root - INFO -   Epoch 56/150	 Time: 5.260	 Loss: 10.66665395
2025-05-21 14:47:27,250 - root - INFO -   Epoch 57/150	 Time: 5.267	 Loss: 10.65770448
2025-05-21 14:47:32,515 - root - INFO -   Epoch 58/150	 Time: 5.265	 Loss: 10.65694057
2025-05-21 14:47:37,798 - root - INFO -   Epoch 59/150	 Time: 5.282	 Loss: 10.64297363
2025-05-21 14:47:43,066 - root - INFO -   Epoch 60/150	 Time: 5.268	 Loss: 10.63540818
2025-05-21 14:47:48,339 - root - INFO -   Epoch 61/150	 Time: 5.273	 Loss: 10.62831533
2025-05-21 14:47:53,618 - root - INFO -   Epoch 62/150	 Time: 5.278	 Loss: 10.61954459
2025-05-21 14:47:58,897 - root - INFO -   Epoch 63/150	 Time: 5.279	 Loss: 10.61798658
2025-05-21 14:48:04,213 - root - INFO -   Epoch 64/150	 Time: 5.316	 Loss: 10.61358798
2025-05-21 14:48:09,508 - root - INFO -   Epoch 65/150	 Time: 5.294	 Loss: 10.60340430
2025-05-21 14:48:14,793 - root - INFO -   Epoch 66/150	 Time: 5.285	 Loss: 10.59984515
2025-05-21 14:48:20,075 - root - INFO -   Epoch 67/150	 Time: 5.281	 Loss: 10.59178747
2025-05-21 14:48:25,356 - root - INFO -   Epoch 68/150	 Time: 5.280	 Loss: 10.58179643
2025-05-21 14:48:30,640 - root - INFO -   Epoch 69/150	 Time: 5.284	 Loss: 10.57814476
2025-05-21 14:48:35,925 - root - INFO -   Epoch 70/150	 Time: 5.285	 Loss: 10.56576178
2025-05-21 14:48:41,212 - root - INFO -   Epoch 71/150	 Time: 5.287	 Loss: 10.55713765
2025-05-21 14:48:46,499 - root - INFO -   Epoch 72/150	 Time: 5.286	 Loss: 10.55948941
2025-05-21 14:48:51,786 - root - INFO -   Epoch 73/150	 Time: 5.287	 Loss: 10.55400659
2025-05-21 14:48:57,070 - root - INFO -   Epoch 74/150	 Time: 5.284	 Loss: 10.54478111
2025-05-21 14:49:02,361 - root - INFO -   Epoch 75/150	 Time: 5.290	 Loss: 10.53448183
2025-05-21 14:49:07,654 - root - INFO -   Epoch 76/150	 Time: 5.293	 Loss: 10.52608670
2025-05-21 14:49:12,965 - root - INFO -   Epoch 77/150	 Time: 5.311	 Loss: 10.51739272
2025-05-21 14:49:18,243 - root - INFO -   Epoch 78/150	 Time: 5.278	 Loss: 10.51215379
2025-05-21 14:49:23,525 - root - INFO -   Epoch 79/150	 Time: 5.282	 Loss: 10.50941391
2025-05-21 14:49:28,810 - root - INFO -   Epoch 80/150	 Time: 5.285	 Loss: 10.50174014
2025-05-21 14:49:34,097 - root - INFO -   Epoch 81/150	 Time: 5.287	 Loss: 10.49501816
2025-05-21 14:49:39,384 - root - INFO -   Epoch 82/150	 Time: 5.286	 Loss: 10.48926847
2025-05-21 14:49:44,660 - root - INFO -   Epoch 83/150	 Time: 5.276	 Loss: 10.48579005
2025-05-21 14:49:49,944 - root - INFO -   Epoch 84/150	 Time: 5.283	 Loss: 10.47485077
2025-05-21 14:49:55,220 - root - INFO -   Epoch 85/150	 Time: 5.276	 Loss: 10.47933086
2025-05-21 14:50:00,502 - root - INFO -   Epoch 86/150	 Time: 5.282	 Loss: 10.46216591
2025-05-21 14:50:05,775 - root - INFO -   Epoch 87/150	 Time: 5.273	 Loss: 10.45637980
2025-05-21 14:50:11,056 - root - INFO -   Epoch 88/150	 Time: 5.281	 Loss: 10.44855304
2025-05-21 14:50:16,343 - root - INFO -   Epoch 89/150	 Time: 5.287	 Loss: 10.44590952
2025-05-21 14:50:21,670 - root - INFO -   Epoch 90/150	 Time: 5.326	 Loss: 10.44089630
2025-05-21 14:50:26,951 - root - INFO -   Epoch 91/150	 Time: 5.281	 Loss: 10.44152694
2025-05-21 14:50:32,237 - root - INFO -   Epoch 92/150	 Time: 5.285	 Loss: 10.43556631
2025-05-21 14:50:37,521 - root - INFO -   Epoch 93/150	 Time: 5.284	 Loss: 10.42625598
2025-05-21 14:50:42,798 - root - INFO -   Epoch 94/150	 Time: 5.277	 Loss: 10.41533429
2025-05-21 14:50:48,082 - root - INFO -   Epoch 95/150	 Time: 5.283	 Loss: 10.41006226
2025-05-21 14:50:53,362 - root - INFO -   Epoch 96/150	 Time: 5.281	 Loss: 10.40949890
2025-05-21 14:50:58,652 - root - INFO -   Epoch 97/150	 Time: 5.289	 Loss: 10.39746629
2025-05-21 14:51:03,943 - root - INFO -   Epoch 98/150	 Time: 5.291	 Loss: 10.39269474
2025-05-21 14:51:09,242 - root - INFO -   Epoch 99/150	 Time: 5.299	 Loss: 10.38167851
2025-05-21 14:51:14,526 - root - INFO -   Epoch 100/150	 Time: 5.284	 Loss: 10.38850334
2025-05-21 14:51:19,823 - root - INFO -   Epoch 101/150	 Time: 5.297	 Loss: 10.36911166
2025-05-21 14:51:25,119 - root - INFO -   Epoch 102/150	 Time: 5.295	 Loss: 10.37362617
2025-05-21 14:51:30,446 - root - INFO -   Epoch 103/150	 Time: 5.328	 Loss: 10.36437156
2025-05-21 14:51:35,740 - root - INFO -   Epoch 104/150	 Time: 5.293	 Loss: 10.35641704
2025-05-21 14:51:41,035 - root - INFO -   Epoch 105/150	 Time: 5.295	 Loss: 10.34950199
2025-05-21 14:51:46,323 - root - INFO -   Epoch 106/150	 Time: 5.287	 Loss: 10.35037084
2025-05-21 14:51:51,611 - root - INFO -   Epoch 107/150	 Time: 5.287	 Loss: 10.33970243
2025-05-21 14:51:56,897 - root - INFO -   Epoch 108/150	 Time: 5.285	 Loss: 10.33550166
2025-05-21 14:52:02,184 - root - INFO -   Epoch 109/150	 Time: 5.286	 Loss: 10.32456545
2025-05-21 14:52:07,480 - root - INFO -   Epoch 110/150	 Time: 5.296	 Loss: 10.32285862
2025-05-21 14:52:12,778 - root - INFO -   Epoch 111/150	 Time: 5.298	 Loss: 10.31686018
2025-05-21 14:52:18,076 - root - INFO -   Epoch 112/150	 Time: 5.297	 Loss: 10.30787211
2025-05-21 14:52:23,366 - root - INFO -   Epoch 113/150	 Time: 5.290	 Loss: 10.30798141
2025-05-21 14:52:28,662 - root - INFO -   Epoch 114/150	 Time: 5.296	 Loss: 10.29699409
2025-05-21 14:52:33,953 - root - INFO -   Epoch 115/150	 Time: 5.291	 Loss: 10.28956942
2025-05-21 14:52:39,281 - root - INFO -   Epoch 116/150	 Time: 5.327	 Loss: 10.28889079
2025-05-21 14:52:44,560 - root - INFO -   Epoch 117/150	 Time: 5.279	 Loss: 10.28056202
2025-05-21 14:52:49,858 - root - INFO -   Epoch 118/150	 Time: 5.298	 Loss: 10.27869581
2025-05-21 14:52:55,147 - root - INFO -   Epoch 119/150	 Time: 5.289	 Loss: 10.27344947
2025-05-21 14:53:00,442 - root - INFO -   Epoch 120/150	 Time: 5.295	 Loss: 10.26546593
2025-05-21 14:53:05,738 - root - INFO -   Epoch 121/150	 Time: 5.296	 Loss: 10.26240731
2025-05-21 14:53:11,026 - root - INFO -   Epoch 122/150	 Time: 5.288	 Loss: 10.25310827
2025-05-21 14:53:16,317 - root - INFO -   Epoch 123/150	 Time: 5.290	 Loss: 10.25343224
2025-05-21 14:53:21,610 - root - INFO -   Epoch 124/150	 Time: 5.293	 Loss: 10.23988367
2025-05-21 14:53:26,911 - root - INFO -   Epoch 125/150	 Time: 5.301	 Loss: 10.23940317
2025-05-21 14:53:32,207 - root - INFO -   Epoch 126/150	 Time: 5.295	 Loss: 10.23424752
2025-05-21 14:53:37,516 - root - INFO -   Epoch 127/150	 Time: 5.309	 Loss: 10.22938834
2025-05-21 14:53:42,825 - root - INFO -   Epoch 128/150	 Time: 5.308	 Loss: 10.22853450
2025-05-21 14:53:48,154 - root - INFO -   Epoch 129/150	 Time: 5.329	 Loss: 10.21581494
2025-05-21 14:53:53,448 - root - INFO -   Epoch 130/150	 Time: 5.294	 Loss: 10.21870133
2025-05-21 14:53:58,731 - root - INFO -   Epoch 131/150	 Time: 5.282	 Loss: 10.20739713
2025-05-21 14:54:04,013 - root - INFO -   Epoch 132/150	 Time: 5.282	 Loss: 10.19779171
2025-05-21 14:54:09,298 - root - INFO -   Epoch 133/150	 Time: 5.285	 Loss: 10.19789032
2025-05-21 14:54:14,571 - root - INFO -   Epoch 134/150	 Time: 5.273	 Loss: 10.18794999
2025-05-21 14:54:19,847 - root - INFO -   Epoch 135/150	 Time: 5.275	 Loss: 10.18607233
2025-05-21 14:54:25,123 - root - INFO -   Epoch 136/150	 Time: 5.276	 Loss: 10.18720111
2025-05-21 14:54:30,400 - root - INFO -   Epoch 137/150	 Time: 5.277	 Loss: 10.17975372
2025-05-21 14:54:35,704 - root - INFO -   Epoch 138/150	 Time: 5.303	 Loss: 10.16970243
2025-05-21 14:54:41,014 - root - INFO -   Epoch 139/150	 Time: 5.310	 Loss: 10.16985771
2025-05-21 14:54:46,314 - root - INFO -   Epoch 140/150	 Time: 5.300	 Loss: 10.16247296
2025-05-21 14:54:51,619 - root - INFO -   Epoch 141/150	 Time: 5.305	 Loss: 10.15924199
2025-05-21 14:54:56,917 - root - INFO -   Epoch 142/150	 Time: 5.298	 Loss: 10.15528508
2025-05-21 14:55:02,215 - root - INFO -   Epoch 143/150	 Time: 5.297	 Loss: 10.14355485
2025-05-21 14:55:07,509 - root - INFO -   Epoch 144/150	 Time: 5.294	 Loss: 10.14148583
2025-05-21 14:55:12,803 - root - INFO -   Epoch 145/150	 Time: 5.294	 Loss: 10.14083549
2025-05-21 14:55:18,100 - root - INFO -   Epoch 146/150	 Time: 5.296	 Loss: 10.13957334
2025-05-21 14:55:23,396 - root - INFO -   Epoch 147/150	 Time: 5.296	 Loss: 10.12583453
2025-05-21 14:55:28,696 - root - INFO -   Epoch 148/150	 Time: 5.301	 Loss: 10.12273213
2025-05-21 14:55:33,999 - root - INFO -   Epoch 149/150	 Time: 5.302	 Loss: 10.11140497
2025-05-21 14:55:39,303 - root - INFO -   Epoch 150/150	 Time: 5.304	 Loss: 10.11283597
2025-05-21 14:55:39,303 - root - INFO - Pretraining time: 792.631
2025-05-21 14:55:39,303 - root - INFO - Finished pretraining.
2025-05-21 14:55:39,303 - root - INFO - Testing autoencoder...
2025-05-21 14:57:39,315 - root - INFO - Log file is ../log/mnist-fashionmnist/log.txt.
2025-05-21 14:57:39,327 - root - INFO - Data path is ../data.
2025-05-21 14:57:39,327 - root - INFO - Export path is ../log/mnist-fashionmnist.
2025-05-21 14:57:39,327 - root - INFO - Dataset: mnist-fashionmnist
2025-05-21 14:57:39,327 - root - INFO - Normal class: [0]
2025-05-21 14:57:39,327 - root - INFO - Conducting cross-dataset experiment...
2025-05-21 14:57:39,327 - root - INFO - Network: mnist_LeNet
2025-05-21 14:57:39,327 - root - INFO - Deep SVDD objective: one-class
2025-05-21 14:57:39,327 - root - INFO - Nu-paramerter: 0.10
2025-05-21 14:57:39,360 - root - INFO - Computation device: cuda
2025-05-21 14:57:39,361 - root - INFO - Number of dataloader workers: 0
2025-05-21 14:57:43,211 - root - INFO - Pretraining: True
2025-05-21 14:57:43,212 - root - INFO - Pretraining optimizer: adam
2025-05-21 14:57:43,212 - root - INFO - Pretraining learning rate: 0.0001
2025-05-21 14:57:43,284 - root - INFO - Pretraining epochs: 150
2025-05-21 14:57:43,285 - root - INFO - Pretraining learning rate scheduler milestones: (50,)
2025-05-21 14:57:43,285 - root - INFO - Pretraining batch size: 200
2025-05-21 14:57:43,285 - root - INFO - Pretraining weight decay: 0.0005
2025-05-21 14:57:47,889 - root - INFO - Starting pretraining...
2025-05-21 15:17:38,939 - root - INFO - Log file is ../log/mnist-fashionmnist/log.txt.
2025-05-21 15:17:38,968 - root - INFO - Data path is ../data.
2025-05-21 15:17:38,968 - root - INFO - Export path is ../log/mnist-fashionmnist.
2025-05-21 15:17:38,968 - root - INFO - Dataset: mnist-fashionmnist
2025-05-21 15:17:38,968 - root - INFO - Normal class: [0]
2025-05-21 15:17:38,968 - root - INFO - Conducting cross-dataset experiment...
2025-05-21 15:17:38,968 - root - INFO - Network: mnist_LeNet
2025-05-21 15:17:38,968 - root - INFO - Deep SVDD objective: one-class
2025-05-21 15:17:38,968 - root - INFO - Nu-paramerter: 0.10
2025-05-21 15:17:38,997 - root - INFO - Computation device: cuda
2025-05-21 15:17:38,997 - root - INFO - Number of dataloader workers: 0
2025-05-21 15:17:39,369 - root - INFO - Pretraining: True
2025-05-21 15:17:39,369 - root - INFO - Pretraining optimizer: adam
2025-05-21 15:17:39,369 - root - INFO - Pretraining learning rate: 0.0001
2025-05-21 15:17:39,369 - root - INFO - Pretraining epochs: 150
2025-05-21 15:17:39,369 - root - INFO - Pretraining learning rate scheduler milestones: (50,)
2025-05-21 15:17:39,369 - root - INFO - Pretraining batch size: 200
2025-05-21 15:17:39,369 - root - INFO - Pretraining weight decay: 0.0005
2025-05-21 15:17:43,013 - root - INFO - Starting pretraining...
2025-05-21 15:22:19,343 - root - INFO - Log file is ../log/mnist-fashionmnist/log.txt.
2025-05-21 15:22:19,356 - root - INFO - Data path is ../data.
2025-05-21 15:22:19,356 - root - INFO - Export path is ../log/mnist-fashionmnist.
2025-05-21 15:22:19,356 - root - INFO - Dataset: mnist-fashionmnist
2025-05-21 15:22:19,356 - root - INFO - Normal class: [0]
2025-05-21 15:22:19,356 - root - INFO - Conducting cross-dataset experiment...
2025-05-21 15:22:19,356 - root - INFO - Network: mnist_LeNet
2025-05-21 15:22:19,357 - root - INFO - Deep SVDD objective: one-class
2025-05-21 15:22:19,357 - root - INFO - Nu-paramerter: 0.10
2025-05-21 15:22:19,408 - root - INFO - Computation device: cuda
2025-05-21 15:22:19,408 - root - INFO - Number of dataloader workers: 0
2025-05-21 15:22:19,657 - root - INFO - Pretraining: True
2025-05-21 15:22:19,657 - root - INFO - Pretraining optimizer: adam
2025-05-21 15:22:19,657 - root - INFO - Pretraining learning rate: 0.0001
2025-05-21 15:22:19,658 - root - INFO - Pretraining epochs: 1
2025-05-21 15:22:19,658 - root - INFO - Pretraining learning rate scheduler milestones: (50,)
2025-05-21 15:22:19,658 - root - INFO - Pretraining batch size: 200
2025-05-21 15:22:19,658 - root - INFO - Pretraining weight decay: 0.0005
2025-05-21 15:22:22,561 - root - INFO - Starting pretraining...
2025-05-21 15:23:18,382 - root - INFO - Log file is ../log/mnist-fashionmnist/log.txt.
2025-05-21 15:23:18,383 - root - INFO - Data path is ../data.
2025-05-21 15:23:18,383 - root - INFO - Export path is ../log/mnist-fashionmnist.
2025-05-21 15:23:18,383 - root - INFO - Dataset: mnist-fashionmnist
2025-05-21 15:23:18,383 - root - INFO - Normal class: [0]
2025-05-21 15:23:18,383 - root - INFO - Conducting cross-dataset experiment...
2025-05-21 15:23:18,383 - root - INFO - Network: mnist_LeNet
2025-05-21 15:23:18,383 - root - INFO - Deep SVDD objective: one-class
2025-05-21 15:23:18,384 - root - INFO - Nu-paramerter: 0.10
2025-05-21 15:23:18,414 - root - INFO - Computation device: cuda
2025-05-21 15:23:18,414 - root - INFO - Number of dataloader workers: 0
2025-05-21 15:23:18,503 - root - INFO - Pretraining: True
2025-05-21 15:23:18,503 - root - INFO - Pretraining optimizer: adam
2025-05-21 15:23:18,503 - root - INFO - Pretraining learning rate: 0.0001
2025-05-21 15:23:18,503 - root - INFO - Pretraining epochs: 1
2025-05-21 15:23:18,503 - root - INFO - Pretraining learning rate scheduler milestones: (50,)
2025-05-21 15:23:18,503 - root - INFO - Pretraining batch size: 200
2025-05-21 15:23:18,503 - root - INFO - Pretraining weight decay: 0.0005
2025-05-21 15:23:21,322 - root - INFO - Starting pretraining...
2025-05-21 15:23:26,855 - root - INFO -   Epoch 1/1	 Time: 5.531	 Loss: 86.60527344
2025-05-21 15:23:26,855 - root - INFO - Pretraining time: 5.532
2025-05-21 15:23:26,855 - root - INFO - Finished pretraining.
2025-05-21 15:23:26,856 - root - INFO - Testing autoencoder...
2025-05-21 15:23:28,260 - root - INFO - Test set Loss: 62.57304073
2025-05-21 15:23:28,273 - root - INFO - Test set AUC: 64.73%
2025-05-21 15:23:28,273 - root - INFO - Autoencoder testing time: 1.417
2025-05-21 15:23:28,273 - root - INFO - Finished testing autoencoder.
2025-05-21 15:23:28,274 - root - INFO - Training optimizer: adam
2025-05-21 15:23:28,275 - root - INFO - Training learning rate: 0.0001
2025-05-21 15:23:28,275 - root - INFO - Training epochs: 1
2025-05-21 15:23:28,275 - root - INFO - Training learning rate scheduler milestones: (50,)
2025-05-21 15:23:28,275 - root - INFO - Training batch size: 200
2025-05-21 15:23:28,275 - root - INFO - Training weight decay: 5e-07
2025-05-21 15:23:28,275 - root - INFO - Initializing center c...
2025-05-21 15:23:32,507 - root - INFO - Center c initialized.
2025-05-21 15:23:32,507 - root - INFO - Starting training...
2025-05-21 15:23:37,254 - root - INFO -   Epoch 1/1	 Time: 4.746	 Loss: 0.49836026
2025-05-21 15:23:37,254 - root - INFO - Training time: 4.746
2025-05-21 15:23:37,254 - root - INFO - Finished training.
2025-05-21 15:23:37,254 - root - INFO - Starting testing...
2025-05-21 15:23:38,609 - root - INFO - Testing time: 1.355
2025-05-21 15:23:38,708 - root - INFO - Test set AUC: 99.21%
2025-05-21 15:23:38,709 - root - INFO - Testing TPR95: 0.9607
2025-05-21 15:23:38,709 - root - INFO - Finished testing.
2025-05-21 15:24:55,935 - root - INFO - Log file is ../log/mnist-fashionmnist/log.txt.
2025-05-21 15:24:55,936 - root - INFO - Data path is ../data.
2025-05-21 15:24:55,936 - root - INFO - Export path is ../log/mnist-fashionmnist.
2025-05-21 15:24:55,936 - root - INFO - Dataset: mnist-fashionmnist
2025-05-21 15:24:55,936 - root - INFO - Normal class: [0]
2025-05-21 15:24:55,936 - root - INFO - Conducting cross-dataset experiment...
2025-05-21 15:24:55,936 - root - INFO - Network: mnist_LeNet
2025-05-21 15:24:55,936 - root - INFO - Deep SVDD objective: one-class
2025-05-21 15:24:55,936 - root - INFO - Nu-paramerter: 0.10
2025-05-21 15:24:55,961 - root - INFO - Computation device: cuda
2025-05-21 15:24:55,962 - root - INFO - Number of dataloader workers: 0
2025-05-21 15:24:56,050 - root - INFO - Pretraining: True
2025-05-21 15:24:56,050 - root - INFO - Pretraining optimizer: adam
2025-05-21 15:24:56,051 - root - INFO - Pretraining learning rate: 0.0001
2025-05-21 15:24:56,051 - root - INFO - Pretraining epochs: 1
2025-05-21 15:24:56,051 - root - INFO - Pretraining learning rate scheduler milestones: (50,)
2025-05-21 15:24:56,051 - root - INFO - Pretraining batch size: 200
2025-05-21 15:24:56,051 - root - INFO - Pretraining weight decay: 0.0005
2025-05-21 15:24:58,843 - root - INFO - Starting pretraining...
2025-05-21 15:25:04,310 - root - INFO -   Epoch 1/1	 Time: 5.466	 Loss: 81.12669120
2025-05-21 15:25:04,310 - root - INFO - Pretraining time: 5.468
2025-05-21 15:25:04,310 - root - INFO - Finished pretraining.
2025-05-21 15:25:04,311 - root - INFO - Testing autoencoder...
2025-05-21 15:25:05,701 - root - INFO - Test set Loss: 63.16275570
2025-05-21 15:25:05,713 - root - INFO - Test set AUC: 70.74%
2025-05-21 15:25:05,714 - root - INFO - Autoencoder testing time: 1.403
2025-05-21 15:25:05,714 - root - INFO - Finished testing autoencoder.
2025-05-21 15:25:05,715 - root - INFO - Training optimizer: adam
2025-05-21 15:25:05,715 - root - INFO - Training learning rate: 0.0001
2025-05-21 15:25:05,716 - root - INFO - Training epochs: 1
2025-05-21 15:25:05,716 - root - INFO - Training learning rate scheduler milestones: (50,)
2025-05-21 15:25:05,718 - root - INFO - Training batch size: 200
2025-05-21 15:25:05,718 - root - INFO - Training weight decay: 5e-07
2025-05-21 15:25:05,718 - root - INFO - Initializing center c...
2025-05-21 15:25:09,881 - root - INFO - Center c initialized.
2025-05-21 15:25:09,881 - root - INFO - Starting training...
2025-05-21 15:25:14,549 - root - INFO -   Epoch 1/1	 Time: 4.668	 Loss: 1.12523365
2025-05-21 15:25:14,550 - root - INFO - Training time: 4.669
2025-05-21 15:25:14,550 - root - INFO - Finished training.
2025-05-21 15:25:14,550 - root - INFO - Starting testing...
2025-05-21 15:25:15,885 - root - INFO - Testing time: 1.335
2025-05-21 15:25:15,984 - root - INFO - Test set AUC: 99.62%
2025-05-21 15:25:15,985 - root - INFO - Testing TPR95: 0.9860
2025-05-21 15:25:15,985 - root - INFO - Finished testing.
2025-05-21 15:25:39,730 - root - INFO - Log file is ../log/mnist-fashionmnist/log.txt.
2025-05-21 15:25:39,731 - root - INFO - Data path is ../data.
2025-05-21 15:25:39,731 - root - INFO - Export path is ../log/mnist-fashionmnist.
2025-05-21 15:25:39,731 - root - INFO - Dataset: mnist-fashionmnist
2025-05-21 15:25:39,731 - root - INFO - Normal class: [0]
2025-05-21 15:25:39,731 - root - INFO - Conducting cross-dataset experiment...
2025-05-21 15:25:39,731 - root - INFO - Network: mnist_LeNet
2025-05-21 15:25:39,731 - root - INFO - Deep SVDD objective: one-class
2025-05-21 15:25:39,732 - root - INFO - Nu-paramerter: 0.10
2025-05-21 15:25:39,758 - root - INFO - Computation device: cuda
2025-05-21 15:25:39,758 - root - INFO - Number of dataloader workers: 0
2025-05-21 15:25:39,846 - root - INFO - Pretraining: True
2025-05-21 15:25:39,846 - root - INFO - Pretraining optimizer: adam
2025-05-21 15:25:39,846 - root - INFO - Pretraining learning rate: 0.0001
2025-05-21 15:25:39,847 - root - INFO - Pretraining epochs: 1
2025-05-21 15:25:39,847 - root - INFO - Pretraining learning rate scheduler milestones: (50,)
2025-05-21 15:25:39,847 - root - INFO - Pretraining batch size: 200
2025-05-21 15:25:39,847 - root - INFO - Pretraining weight decay: 0.0005
2025-05-21 15:25:42,649 - root - INFO - Starting pretraining...
2025-05-21 15:25:48,134 - root - INFO -   Epoch 1/1	 Time: 5.483	 Loss: 66.23981518
2025-05-21 15:25:48,134 - root - INFO - Pretraining time: 5.484
2025-05-21 15:25:48,134 - root - INFO - Finished pretraining.
2025-05-21 15:25:48,135 - root - INFO - Testing autoencoder...
2025-05-21 15:25:49,522 - root - INFO - Test set Loss: 63.00320339
2025-05-21 15:25:49,534 - root - INFO - Test set AUC: 71.09%
2025-05-21 15:25:49,535 - root - INFO - Autoencoder testing time: 1.400
2025-05-21 15:25:49,535 - root - INFO - Finished testing autoencoder.
2025-05-21 15:25:49,536 - root - INFO - Training optimizer: adam
2025-05-21 15:25:49,536 - root - INFO - Training learning rate: 0.0001
2025-05-21 15:25:49,536 - root - INFO - Training epochs: 1
2025-05-21 15:25:49,537 - root - INFO - Training learning rate scheduler milestones: (50,)
2025-05-21 15:25:49,537 - root - INFO - Training batch size: 200
2025-05-21 15:25:49,537 - root - INFO - Training weight decay: 5e-07
2025-05-21 15:25:49,537 - root - INFO - Initializing center c...
2025-05-21 15:25:53,723 - root - INFO - Center c initialized.
2025-05-21 15:25:53,723 - root - INFO - Starting training...
2025-05-21 15:25:58,405 - root - INFO -   Epoch 1/1	 Time: 4.682	 Loss: 0.41394832
2025-05-21 15:25:58,406 - root - INFO - Training time: 4.683
2025-05-21 15:25:58,406 - root - INFO - Finished training.
2025-05-21 15:25:58,406 - root - INFO - Starting testing...
2025-05-21 15:25:59,738 - root - INFO - Testing time: 1.332
2025-05-21 15:25:59,837 - root - INFO - Test set AUC: 99.05%
2025-05-21 15:25:59,838 - root - INFO - Testing TPR95: 0.9460
2025-05-21 15:25:59,838 - root - INFO - Finished testing.
2025-05-21 15:29:03,757 - root - INFO - Log file is ../log/mnist-fashionmnist/log.txt.
2025-05-21 15:29:03,758 - root - INFO - Data path is ../data.
2025-05-21 15:29:03,758 - root - INFO - Export path is ../log/mnist-fashionmnist.
2025-05-21 15:29:03,758 - root - INFO - Dataset: mnist-fashionmnist
2025-05-21 15:29:03,758 - root - INFO - Normal class: [0]
2025-05-21 15:29:03,758 - root - INFO - Conducting cross-dataset experiment...
2025-05-21 15:29:03,758 - root - INFO - Network: mnist_LeNet
2025-05-21 15:29:03,758 - root - INFO - Deep SVDD objective: one-class
2025-05-21 15:29:03,758 - root - INFO - Nu-paramerter: 0.10
2025-05-21 15:29:03,783 - root - INFO - Computation device: cuda
2025-05-21 15:29:03,784 - root - INFO - Number of dataloader workers: 0
2025-05-21 15:29:03,872 - root - INFO - Pretraining: True
2025-05-21 15:29:03,872 - root - INFO - Pretraining optimizer: adam
2025-05-21 15:29:03,872 - root - INFO - Pretraining learning rate: 0.0001
2025-05-21 15:29:03,873 - root - INFO - Pretraining epochs: 1
2025-05-21 15:29:03,873 - root - INFO - Pretraining learning rate scheduler milestones: (50,)
2025-05-21 15:29:03,873 - root - INFO - Pretraining batch size: 200
2025-05-21 15:29:03,873 - root - INFO - Pretraining weight decay: 0.0005
2025-05-21 15:29:06,666 - root - INFO - Starting pretraining...
2025-05-21 15:29:12,153 - root - INFO -   Epoch 1/1	 Time: 5.485	 Loss: 167.60388357
2025-05-21 15:29:12,153 - root - INFO - Pretraining time: 5.486
2025-05-21 15:29:12,153 - root - INFO - Finished pretraining.
2025-05-21 15:29:12,154 - root - INFO - Testing autoencoder...
2025-05-21 15:29:13,545 - root - INFO - Test set Loss: 79.37086716
2025-05-21 15:29:13,557 - root - INFO - Test set AUC: 43.01%
2025-05-21 15:29:13,558 - root - INFO - Autoencoder testing time: 1.404
2025-05-21 15:29:13,558 - root - INFO - Finished testing autoencoder.
2025-05-21 15:29:13,559 - root - INFO - Training optimizer: adam
2025-05-21 15:29:13,559 - root - INFO - Training learning rate: 0.0001
2025-05-21 15:29:13,559 - root - INFO - Training epochs: 1
2025-05-21 15:29:13,559 - root - INFO - Training learning rate scheduler milestones: (50,)
2025-05-21 15:29:13,560 - root - INFO - Training batch size: 200
2025-05-21 15:29:13,560 - root - INFO - Training weight decay: 5e-07
2025-05-21 15:29:13,560 - root - INFO - Initializing center c...
2025-05-21 15:29:17,730 - root - INFO - Center c initialized.
2025-05-21 15:29:17,730 - root - INFO - Starting training...
2025-05-21 15:29:22,406 - root - INFO -   Epoch 1/1	 Time: 4.675	 Loss: 0.98094798
2025-05-21 15:29:22,406 - root - INFO - Training time: 4.676
2025-05-21 15:29:22,406 - root - INFO - Finished training.
2025-05-21 15:29:22,406 - root - INFO - Starting testing...
2025-05-21 15:29:23,743 - root - INFO - Testing time: 1.336
2025-05-21 15:29:23,842 - root - INFO - Test set AUC: 98.37%
2025-05-21 15:29:23,843 - root - INFO - Testing TPR95: 0.9026
2025-05-21 15:29:23,843 - root - INFO - Finished testing.
2025-05-21 15:29:57,303 - root - INFO - Log file is ../log/mnist-fashionmnist/log.txt.
2025-05-21 15:29:57,324 - root - INFO - Data path is ../data.
2025-05-21 15:29:57,324 - root - INFO - Export path is ../log/mnist-fashionmnist.
2025-05-21 15:29:57,324 - root - INFO - Dataset: mnist-fashionmnist
2025-05-21 15:29:57,324 - root - INFO - Normal class: [0]
2025-05-21 15:29:57,324 - root - INFO - Conducting cross-dataset experiment...
2025-05-21 15:29:57,324 - root - INFO - Network: mnist_LeNet
2025-05-21 15:29:57,324 - root - INFO - Deep SVDD objective: one-class
2025-05-21 15:29:57,324 - root - INFO - Nu-paramerter: 0.10
2025-05-21 15:29:57,354 - root - INFO - Computation device: cuda
2025-05-21 15:29:57,354 - root - INFO - Number of dataloader workers: 0
2025-05-21 15:29:57,735 - root - INFO - Pretraining: True
2025-05-21 15:29:57,736 - root - INFO - Pretraining optimizer: adam
2025-05-21 15:29:57,736 - root - INFO - Pretraining learning rate: 0.0001
2025-05-21 15:29:57,736 - root - INFO - Pretraining epochs: 150
2025-05-21 15:29:57,736 - root - INFO - Pretraining learning rate scheduler milestones: (50,)
2025-05-21 15:29:57,736 - root - INFO - Pretraining batch size: 200
2025-05-21 15:29:57,736 - root - INFO - Pretraining weight decay: 0.0005
2025-05-21 15:30:02,380 - root - INFO - Starting pretraining...
2025-05-21 15:30:09,405 - root - INFO -   Epoch 1/150	 Time: 7.021	 Loss: 103.85025639
2025-05-21 15:30:15,775 - root - INFO -   Epoch 2/150	 Time: 6.314	 Loss: 51.02660765
2025-05-21 15:30:22,076 - root - INFO -   Epoch 3/150	 Time: 6.300	 Loss: 42.42823352
2025-05-21 15:30:28,351 - root - INFO -   Epoch 4/150	 Time: 6.275	 Loss: 36.91437012
2025-05-21 15:30:34,666 - root - INFO -   Epoch 5/150	 Time: 6.314	 Loss: 32.11148308
2025-05-21 15:30:40,943 - root - INFO -   Epoch 6/150	 Time: 6.277	 Loss: 28.48115472
2025-05-21 15:30:47,224 - root - INFO -   Epoch 7/150	 Time: 6.279	 Loss: 25.68605117
2025-05-21 15:30:53,526 - root - INFO -   Epoch 8/150	 Time: 6.301	 Loss: 23.49118938
2025-05-21 15:30:59,864 - root - INFO -   Epoch 9/150	 Time: 6.337	 Loss: 21.75323856
2025-05-21 15:31:06,145 - root - INFO -   Epoch 10/150	 Time: 6.281	 Loss: 20.35331519
2025-05-21 15:31:12,418 - root - INFO -   Epoch 11/150	 Time: 6.272	 Loss: 19.20208316
2025-05-21 15:31:18,722 - root - INFO -   Epoch 12/150	 Time: 6.304	 Loss: 18.23394409
2025-05-21 15:31:25,079 - root - INFO -   Epoch 13/150	 Time: 6.357	 Loss: 17.39708931
2025-05-21 15:31:31,382 - root - INFO -   Epoch 14/150	 Time: 6.303	 Loss: 16.71153128
2025-05-21 15:31:37,665 - root - INFO -   Epoch 15/150	 Time: 6.283	 Loss: 16.14938024
2025-05-21 15:31:43,945 - root - INFO -   Epoch 16/150	 Time: 6.280	 Loss: 15.68400619
2025-05-21 15:31:50,245 - root - INFO -   Epoch 17/150	 Time: 6.300	 Loss: 15.28615712
2025-05-21 15:31:56,587 - root - INFO -   Epoch 18/150	 Time: 6.341	 Loss: 14.92271763
2025-05-21 15:32:02,881 - root - INFO -   Epoch 19/150	 Time: 6.293	 Loss: 14.59530350
2025-05-21 15:32:09,155 - root - INFO -   Epoch 20/150	 Time: 6.274	 Loss: 14.30643187
2025-05-21 15:32:15,440 - root - INFO -   Epoch 21/150	 Time: 6.285	 Loss: 14.04476919
2025-05-21 15:32:21,737 - root - INFO -   Epoch 22/150	 Time: 6.297	 Loss: 13.78596216
2025-05-21 15:32:28,023 - root - INFO -   Epoch 23/150	 Time: 6.286	 Loss: 13.52322661
2025-05-21 15:32:34,354 - root - INFO -   Epoch 24/150	 Time: 6.330	 Loss: 13.29900810
2025-05-21 15:32:40,631 - root - INFO -   Epoch 25/150	 Time: 6.277	 Loss: 13.08910203
2025-05-21 15:32:46,912 - root - INFO -   Epoch 26/150	 Time: 6.280	 Loss: 12.88995081
2025-05-21 15:32:53,202 - root - INFO -   Epoch 27/150	 Time: 6.290	 Loss: 12.69327706
2025-05-21 15:32:59,538 - root - INFO -   Epoch 28/150	 Time: 6.336	 Loss: 12.52956826
2025-05-21 15:33:05,836 - root - INFO -   Epoch 29/150	 Time: 6.298	 Loss: 12.37202870
2025-05-21 15:33:12,146 - root - INFO -   Epoch 30/150	 Time: 6.310	 Loss: 12.20438425
2025-05-21 15:33:18,494 - root - INFO -   Epoch 31/150	 Time: 6.348	 Loss: 12.07588117
2025-05-21 15:33:24,793 - root - INFO -   Epoch 32/150	 Time: 6.298	 Loss: 11.94335675
2025-05-21 15:33:31,091 - root - INFO -   Epoch 33/150	 Time: 6.298	 Loss: 11.81419088
2025-05-21 15:33:37,391 - root - INFO -   Epoch 34/150	 Time: 6.300	 Loss: 11.68704852
2025-05-21 15:33:43,680 - root - INFO -   Epoch 35/150	 Time: 6.289	 Loss: 11.56110422
2025-05-21 15:33:49,971 - root - INFO -   Epoch 36/150	 Time: 6.291	 Loss: 11.46965814
2025-05-21 15:33:56,263 - root - INFO -   Epoch 37/150	 Time: 6.291	 Loss: 11.36095059
2025-05-21 15:34:02,566 - root - INFO -   Epoch 38/150	 Time: 6.303	 Loss: 11.26645699
2025-05-21 15:34:08,882 - root - INFO -   Epoch 39/150	 Time: 6.316	 Loss: 11.16060301
2025-05-21 15:34:15,232 - root - INFO -   Epoch 40/150	 Time: 6.348	 Loss: 11.07134689
2025-05-21 15:34:21,543 - root - INFO -   Epoch 41/150	 Time: 6.310	 Loss: 10.99007197
2025-05-21 15:34:27,858 - root - INFO -   Epoch 42/150	 Time: 6.315	 Loss: 10.91593138
2025-05-21 15:34:34,171 - root - INFO -   Epoch 43/150	 Time: 6.313	 Loss: 10.83016823
2025-05-21 15:34:40,482 - root - INFO -   Epoch 44/150	 Time: 6.311	 Loss: 10.76166467
2025-05-21 15:34:46,779 - root - INFO -   Epoch 45/150	 Time: 6.296	 Loss: 10.70283312
2025-05-21 15:34:53,081 - root - INFO -   Epoch 46/150	 Time: 6.302	 Loss: 10.62792706
2025-05-21 15:34:59,375 - root - INFO -   Epoch 47/150	 Time: 6.294	 Loss: 10.56111320
2025-05-21 15:35:05,671 - root - INFO -   Epoch 48/150	 Time: 6.296	 Loss: 10.49648727
2025-05-21 15:35:11,976 - root - INFO -   Epoch 49/150	 Time: 6.305	 Loss: 10.43205914
2025-05-21 15:35:18,328 - root - INFO -   Epoch 50/150	 Time: 6.352	 Loss: 10.33543114
2025-05-21 15:35:18,329 - root - INFO -   LR scheduler: new learning rate is 1e-05
2025-05-21 15:35:24,641 - root - INFO -   Epoch 51/150	 Time: 6.312	 Loss: 10.33011085
2025-05-21 15:35:30,959 - root - INFO -   Epoch 52/150	 Time: 6.318	 Loss: 10.31762621
2025-05-21 15:35:37,263 - root - INFO -   Epoch 53/150	 Time: 6.304	 Loss: 10.31552771
2025-05-21 15:35:43,569 - root - INFO -   Epoch 54/150	 Time: 6.306	 Loss: 10.30950544
2025-05-21 15:35:49,884 - root - INFO -   Epoch 55/150	 Time: 6.314	 Loss: 10.29546312
2025-05-21 15:35:56,199 - root - INFO -   Epoch 56/150	 Time: 6.314	 Loss: 10.29246577
2025-05-21 15:36:02,521 - root - INFO -   Epoch 57/150	 Time: 6.322	 Loss: 10.28931500
2025-05-21 15:36:08,835 - root - INFO -   Epoch 58/150	 Time: 6.314	 Loss: 10.26992084
2025-05-21 15:36:15,136 - root - INFO -   Epoch 59/150	 Time: 6.301	 Loss: 10.27065815
2025-05-21 15:36:21,456 - root - INFO -   Epoch 60/150	 Time: 6.320	 Loss: 10.26390020
2025-05-21 15:36:27,815 - root - INFO -   Epoch 61/150	 Time: 6.358	 Loss: 10.24970172
2025-05-21 15:36:34,123 - root - INFO -   Epoch 62/150	 Time: 6.308	 Loss: 10.24209973
2025-05-21 15:36:40,446 - root - INFO -   Epoch 63/150	 Time: 6.323	 Loss: 10.24391004
2025-05-21 15:36:46,760 - root - INFO -   Epoch 64/150	 Time: 6.314	 Loss: 10.24126881
2025-05-21 15:36:53,069 - root - INFO -   Epoch 65/150	 Time: 6.309	 Loss: 10.23420887
2025-05-21 15:36:59,387 - root - INFO -   Epoch 66/150	 Time: 6.317	 Loss: 10.21476748
2025-05-21 15:37:05,703 - root - INFO -   Epoch 67/150	 Time: 6.316	 Loss: 10.21004686
2025-05-21 15:37:12,044 - root - INFO -   Epoch 68/150	 Time: 6.341	 Loss: 10.20490323
2025-05-21 15:37:18,371 - root - INFO -   Epoch 69/150	 Time: 6.327	 Loss: 10.19931743
2025-05-21 15:37:24,688 - root - INFO -   Epoch 70/150	 Time: 6.317	 Loss: 10.18212899
2025-05-21 15:37:31,029 - root - INFO -   Epoch 71/150	 Time: 6.340	 Loss: 10.18328396
2025-05-21 15:37:37,367 - root - INFO -   Epoch 72/150	 Time: 6.338	 Loss: 10.16940943
2025-05-21 15:37:43,701 - root - INFO -   Epoch 73/150	 Time: 6.334	 Loss: 10.16707386
2025-05-21 15:37:50,024 - root - INFO -   Epoch 74/150	 Time: 6.323	 Loss: 10.15484548
2025-05-21 15:37:56,335 - root - INFO -   Epoch 75/150	 Time: 6.310	 Loss: 10.15435200
2025-05-21 15:38:02,639 - root - INFO -   Epoch 76/150	 Time: 6.304	 Loss: 10.14529806
2025-05-21 15:38:08,955 - root - INFO -   Epoch 77/150	 Time: 6.316	 Loss: 10.13229398
2025-05-21 15:38:15,268 - root - INFO -   Epoch 78/150	 Time: 6.313	 Loss: 10.13263208
2025-05-21 15:38:21,594 - root - INFO -   Epoch 79/150	 Time: 6.326	 Loss: 10.12391773
2025-05-21 15:38:27,903 - root - INFO -   Epoch 80/150	 Time: 6.308	 Loss: 10.11883592
2025-05-21 15:38:34,215 - root - INFO -   Epoch 81/150	 Time: 6.312	 Loss: 10.10929118
2025-05-21 15:38:40,545 - root - INFO -   Epoch 82/150	 Time: 6.330	 Loss: 10.10596559
2025-05-21 15:38:46,853 - root - INFO -   Epoch 83/150	 Time: 6.307	 Loss: 10.08906347
2025-05-21 15:38:53,162 - root - INFO -   Epoch 84/150	 Time: 6.309	 Loss: 10.08849126
2025-05-21 15:38:59,457 - root - INFO -   Epoch 85/150	 Time: 6.295	 Loss: 10.08624823
2025-05-21 15:39:05,756 - root - INFO -   Epoch 86/150	 Time: 6.298	 Loss: 10.07200619
2025-05-21 15:39:12,057 - root - INFO -   Epoch 87/150	 Time: 6.301	 Loss: 10.05607551
2025-05-21 15:39:18,359 - root - INFO -   Epoch 88/150	 Time: 6.302	 Loss: 10.05485999
2025-05-21 15:39:24,667 - root - INFO -   Epoch 89/150	 Time: 6.308	 Loss: 10.04849738
2025-05-21 15:39:30,976 - root - INFO -   Epoch 90/150	 Time: 6.309	 Loss: 10.04137314
2025-05-21 15:39:37,291 - root - INFO -   Epoch 91/150	 Time: 6.315	 Loss: 10.04280376
2025-05-21 15:39:43,621 - root - INFO -   Epoch 92/150	 Time: 6.330	 Loss: 10.03901103
2025-05-21 15:39:49,979 - root - INFO -   Epoch 93/150	 Time: 6.358	 Loss: 10.02791939
2025-05-21 15:39:56,290 - root - INFO -   Epoch 94/150	 Time: 6.310	 Loss: 10.01838212
2025-05-21 15:40:02,588 - root - INFO -   Epoch 95/150	 Time: 6.298	 Loss: 10.00798127
2025-05-21 15:40:08,893 - root - INFO -   Epoch 96/150	 Time: 6.305	 Loss: 10.00261164
2025-05-21 15:40:15,203 - root - INFO -   Epoch 97/150	 Time: 6.309	 Loss: 9.99915450
2025-05-21 15:40:21,515 - root - INFO -   Epoch 98/150	 Time: 6.312	 Loss: 9.99190571
2025-05-21 15:40:27,827 - root - INFO -   Epoch 99/150	 Time: 6.312	 Loss: 9.98832252
2025-05-21 15:40:34,136 - root - INFO -   Epoch 100/150	 Time: 6.309	 Loss: 9.97781340
2025-05-21 15:40:40,450 - root - INFO -   Epoch 101/150	 Time: 6.313	 Loss: 9.96608983
2025-05-21 15:40:46,773 - root - INFO -   Epoch 102/150	 Time: 6.323	 Loss: 9.96540575
2025-05-21 15:40:53,102 - root - INFO -   Epoch 103/150	 Time: 6.329	 Loss: 9.95342198
2025-05-21 15:40:59,461 - root - INFO -   Epoch 104/150	 Time: 6.358	 Loss: 9.95620439
2025-05-21 15:41:05,782 - root - INFO -   Epoch 105/150	 Time: 6.321	 Loss: 9.94680767
2025-05-21 15:41:12,096 - root - INFO -   Epoch 106/150	 Time: 6.314	 Loss: 9.94626193
2025-05-21 15:41:18,412 - root - INFO -   Epoch 107/150	 Time: 6.316	 Loss: 9.93627349
2025-05-21 15:41:24,742 - root - INFO -   Epoch 108/150	 Time: 6.330	 Loss: 9.92470448
2025-05-21 15:41:31,071 - root - INFO -   Epoch 109/150	 Time: 6.329	 Loss: 9.91751751
2025-05-21 15:41:37,407 - root - INFO -   Epoch 110/150	 Time: 6.336	 Loss: 9.91553864
2025-05-21 15:41:43,737 - root - INFO -   Epoch 111/150	 Time: 6.329	 Loss: 9.90963295
2025-05-21 15:41:50,054 - root - INFO -   Epoch 112/150	 Time: 6.317	 Loss: 9.89731020
2025-05-21 15:41:56,382 - root - INFO -   Epoch 113/150	 Time: 6.327	 Loss: 9.90842252
2025-05-21 15:42:02,726 - root - INFO -   Epoch 114/150	 Time: 6.344	 Loss: 9.89028135
2025-05-21 15:42:09,095 - root - INFO -   Epoch 115/150	 Time: 6.368	 Loss: 9.88584599
2025-05-21 15:42:15,430 - root - INFO -   Epoch 116/150	 Time: 6.334	 Loss: 9.88085849
2025-05-21 15:42:21,771 - root - INFO -   Epoch 117/150	 Time: 6.341	 Loss: 9.86944732
2025-05-21 15:42:28,111 - root - INFO -   Epoch 118/150	 Time: 6.340	 Loss: 9.87336254
2025-05-21 15:42:34,436 - root - INFO -   Epoch 119/150	 Time: 6.325	 Loss: 9.85961783
2025-05-21 15:42:40,761 - root - INFO -   Epoch 120/150	 Time: 6.325	 Loss: 9.85909640
2025-05-21 15:42:47,084 - root - INFO -   Epoch 121/150	 Time: 6.323	 Loss: 9.85169363
2025-05-21 15:42:53,420 - root - INFO -   Epoch 122/150	 Time: 6.335	 Loss: 9.84350052
2025-05-21 15:42:59,764 - root - INFO -   Epoch 123/150	 Time: 6.344	 Loss: 9.84170459
2025-05-21 15:43:06,105 - root - INFO -   Epoch 124/150	 Time: 6.341	 Loss: 9.83422058
2025-05-21 15:43:12,475 - root - INFO -   Epoch 125/150	 Time: 6.369	 Loss: 9.82513343
2025-05-21 15:43:18,847 - root - INFO -   Epoch 126/150	 Time: 6.372	 Loss: 9.82024409
2025-05-21 15:43:25,175 - root - INFO -   Epoch 127/150	 Time: 6.327	 Loss: 9.81702079
2025-05-21 15:43:31,498 - root - INFO -   Epoch 128/150	 Time: 6.322	 Loss: 9.80855814
2025-05-21 15:43:37,824 - root - INFO -   Epoch 129/150	 Time: 6.326	 Loss: 9.80610751
2025-05-21 15:43:44,153 - root - INFO -   Epoch 130/150	 Time: 6.329	 Loss: 9.79449410
2025-05-21 15:43:50,502 - root - INFO -   Epoch 131/150	 Time: 6.348	 Loss: 9.79438194
2025-05-21 15:43:56,832 - root - INFO -   Epoch 132/150	 Time: 6.330	 Loss: 9.78836874
2025-05-21 15:44:03,164 - root - INFO -   Epoch 133/150	 Time: 6.332	 Loss: 9.78165918
2025-05-21 15:44:09,497 - root - INFO -   Epoch 134/150	 Time: 6.333	 Loss: 9.77998025
2025-05-21 15:44:15,835 - root - INFO -   Epoch 135/150	 Time: 6.337	 Loss: 9.77337128
2025-05-21 15:44:22,198 - root - INFO -   Epoch 136/150	 Time: 6.363	 Loss: 9.76485286
2025-05-21 15:44:28,575 - root - INFO -   Epoch 137/150	 Time: 6.377	 Loss: 9.75908969
2025-05-21 15:44:34,903 - root - INFO -   Epoch 138/150	 Time: 6.328	 Loss: 9.75804515
2025-05-21 15:44:41,227 - root - INFO -   Epoch 139/150	 Time: 6.323	 Loss: 9.74988522
2025-05-21 15:44:47,552 - root - INFO -   Epoch 140/150	 Time: 6.325	 Loss: 9.74913821
2025-05-21 15:44:53,892 - root - INFO -   Epoch 141/150	 Time: 6.340	 Loss: 9.73325370
2025-05-21 15:45:00,220 - root - INFO -   Epoch 142/150	 Time: 6.328	 Loss: 9.72599778
2025-05-21 15:45:06,549 - root - INFO -   Epoch 143/150	 Time: 6.328	 Loss: 9.73156742
2025-05-21 15:45:12,872 - root - INFO -   Epoch 144/150	 Time: 6.323	 Loss: 9.72078548
2025-05-21 15:45:19,211 - root - INFO -   Epoch 145/150	 Time: 6.339	 Loss: 9.71313258
2025-05-21 15:45:25,552 - root - INFO -   Epoch 146/150	 Time: 6.341	 Loss: 9.71655648
2025-05-21 15:45:31,936 - root - INFO -   Epoch 147/150	 Time: 6.383	 Loss: 9.70909270
2025-05-21 15:45:38,295 - root - INFO -   Epoch 148/150	 Time: 6.359	 Loss: 9.69352814
2025-05-21 15:45:44,633 - root - INFO -   Epoch 149/150	 Time: 6.338	 Loss: 9.69868362
2025-05-21 15:45:50,964 - root - INFO -   Epoch 150/150	 Time: 6.331	 Loss: 9.69184446
2025-05-21 15:45:50,965 - root - INFO - Pretraining time: 948.583
2025-05-21 15:45:50,965 - root - INFO - Finished pretraining.
2025-05-21 15:45:50,965 - root - INFO - Testing autoencoder...
2025-05-21 15:45:52,672 - root - INFO - Test set Loss: 34.76947791
2025-05-21 15:45:52,688 - root - INFO - Test set AUC: 99.20%
2025-05-21 15:45:52,688 - root - INFO - Autoencoder testing time: 1.723
2025-05-21 15:45:52,688 - root - INFO - Finished testing autoencoder.
2025-05-21 15:45:52,690 - root - INFO - Training optimizer: adam
2025-05-21 15:45:52,690 - root - INFO - Training learning rate: 0.0001
2025-05-21 15:45:52,690 - root - INFO - Training epochs: 150
2025-05-21 15:45:52,691 - root - INFO - Training learning rate scheduler milestones: (50,)
2025-05-21 15:45:52,691 - root - INFO - Training batch size: 200
2025-05-21 15:45:52,691 - root - INFO - Training weight decay: 5e-07
2025-05-21 15:45:52,691 - root - INFO - Initializing center c...
2025-05-21 15:45:57,962 - root - INFO - Center c initialized.
2025-05-21 15:45:57,963 - root - INFO - Starting training...
2025-05-21 15:46:03,735 - root - INFO -   Epoch 1/150	 Time: 5.773	 Loss: 1.79778797
2025-05-21 15:46:09,494 - root - INFO -   Epoch 2/150	 Time: 5.758	 Loss: 0.37149828
2025-05-21 15:46:15,250 - root - INFO -   Epoch 3/150	 Time: 5.756	 Loss: 0.19635517
2025-05-21 15:46:21,013 - root - INFO -   Epoch 4/150	 Time: 5.763	 Loss: 0.13084501
2025-05-21 15:46:26,773 - root - INFO -   Epoch 5/150	 Time: 5.759	 Loss: 0.09954457
2025-05-21 15:46:32,530 - root - INFO -   Epoch 6/150	 Time: 5.757	 Loss: 0.07977155
2025-05-21 15:46:38,301 - root - INFO -   Epoch 7/150	 Time: 5.771	 Loss: 0.06662303
2025-05-21 15:46:44,124 - root - INFO -   Epoch 8/150	 Time: 5.822	 Loss: 0.05809385
2025-05-21 15:46:49,879 - root - INFO -   Epoch 9/150	 Time: 5.754	 Loss: 0.04895328
2025-05-21 15:46:55,624 - root - INFO -   Epoch 10/150	 Time: 5.745	 Loss: 0.04423547
2025-05-21 15:47:01,366 - root - INFO -   Epoch 11/150	 Time: 5.742	 Loss: 0.03887729
2025-05-21 15:47:07,114 - root - INFO -   Epoch 12/150	 Time: 5.748	 Loss: 0.03641823
2025-05-21 15:47:12,879 - root - INFO -   Epoch 13/150	 Time: 5.764	 Loss: 0.03194926
2025-05-21 15:47:18,664 - root - INFO -   Epoch 14/150	 Time: 5.785	 Loss: 0.03019968
2025-05-21 15:47:24,435 - root - INFO -   Epoch 15/150	 Time: 5.771	 Loss: 0.02775674
2025-05-21 15:47:30,196 - root - INFO -   Epoch 16/150	 Time: 5.760	 Loss: 0.02555297
2025-05-21 15:47:35,966 - root - INFO -   Epoch 17/150	 Time: 5.770	 Loss: 0.02299633
2025-05-21 15:47:41,766 - root - INFO -   Epoch 18/150	 Time: 5.799	 Loss: 0.02134726
2025-05-21 15:47:47,589 - root - INFO -   Epoch 19/150	 Time: 5.822	 Loss: 0.02060265
2025-05-21 15:47:53,374 - root - INFO -   Epoch 20/150	 Time: 5.785	 Loss: 0.01976903
2025-05-21 15:47:59,130 - root - INFO -   Epoch 21/150	 Time: 5.755	 Loss: 0.01800255
2025-05-21 15:48:04,877 - root - INFO -   Epoch 22/150	 Time: 5.747	 Loss: 0.01654033
2025-05-21 15:48:10,630 - root - INFO -   Epoch 23/150	 Time: 5.752	 Loss: 0.01567833
2025-05-21 15:48:16,385 - root - INFO -   Epoch 24/150	 Time: 5.755	 Loss: 0.01440637
2025-05-21 15:48:22,150 - root - INFO -   Epoch 25/150	 Time: 5.765	 Loss: 0.01390781
2025-05-21 15:48:27,906 - root - INFO -   Epoch 26/150	 Time: 5.756	 Loss: 0.01306043
2025-05-21 15:48:33,665 - root - INFO -   Epoch 27/150	 Time: 5.759	 Loss: 0.01305436
2025-05-21 15:48:39,418 - root - INFO -   Epoch 28/150	 Time: 5.753	 Loss: 0.01190431
2025-05-21 15:48:45,173 - root - INFO -   Epoch 29/150	 Time: 5.754	 Loss: 0.01077952
2025-05-21 15:48:50,959 - root - INFO -   Epoch 30/150	 Time: 5.786	 Loss: 0.01090285
2025-05-21 15:48:56,771 - root - INFO -   Epoch 31/150	 Time: 5.812	 Loss: 0.00998206
2025-05-21 15:49:02,528 - root - INFO -   Epoch 32/150	 Time: 5.756	 Loss: 0.00967911
2025-05-21 15:49:08,286 - root - INFO -   Epoch 33/150	 Time: 5.757	 Loss: 0.00874324
2025-05-21 15:49:14,035 - root - INFO -   Epoch 34/150	 Time: 5.749	 Loss: 0.00868770
2025-05-21 15:49:19,799 - root - INFO -   Epoch 35/150	 Time: 5.763	 Loss: 0.00817053
2025-05-21 15:49:25,566 - root - INFO -   Epoch 36/150	 Time: 5.767	 Loss: 0.00779020
2025-05-21 15:49:31,334 - root - INFO -   Epoch 37/150	 Time: 5.767	 Loss: 0.00716844
2025-05-21 15:49:37,092 - root - INFO -   Epoch 38/150	 Time: 5.758	 Loss: 0.00711654
2025-05-21 15:49:42,856 - root - INFO -   Epoch 39/150	 Time: 5.763	 Loss: 0.00661696
2025-05-21 15:49:48,624 - root - INFO -   Epoch 40/150	 Time: 5.768	 Loss: 0.00640102
2025-05-21 15:49:54,408 - root - INFO -   Epoch 41/150	 Time: 5.784	 Loss: 0.00596264
2025-05-21 15:50:00,207 - root - INFO -   Epoch 42/150	 Time: 5.798	 Loss: 0.00579648
2025-05-21 15:50:06,026 - root - INFO -   Epoch 43/150	 Time: 5.819	 Loss: 0.00568682
2025-05-21 15:50:11,788 - root - INFO -   Epoch 44/150	 Time: 5.761	 Loss: 0.00508345
2025-05-21 15:50:17,558 - root - INFO -   Epoch 45/150	 Time: 5.770	 Loss: 0.00482726
2025-05-21 15:50:23,337 - root - INFO -   Epoch 46/150	 Time: 5.778	 Loss: 0.00487526
2025-05-21 15:50:29,119 - root - INFO -   Epoch 47/150	 Time: 5.782	 Loss: 0.00452893
2025-05-21 15:50:34,895 - root - INFO -   Epoch 48/150	 Time: 5.776	 Loss: 0.00437497
2025-05-21 15:50:40,668 - root - INFO -   Epoch 49/150	 Time: 5.772	 Loss: 0.00416808
2025-05-21 15:50:46,438 - root - INFO -   Epoch 50/150	 Time: 5.770	 Loss: 0.00374837
2025-05-21 15:50:46,438 - root - INFO -   LR scheduler: new learning rate is 1e-05
2025-05-21 15:50:52,220 - root - INFO -   Epoch 51/150	 Time: 5.782	 Loss: 0.00370470
2025-05-21 15:50:57,996 - root - INFO -   Epoch 52/150	 Time: 5.776	 Loss: 0.00361884
2025-05-21 15:51:03,787 - root - INFO -   Epoch 53/150	 Time: 5.790	 Loss: 0.00364363
2025-05-21 15:51:09,611 - root - INFO -   Epoch 54/150	 Time: 5.824	 Loss: 0.00369048
2025-05-21 15:51:15,386 - root - INFO -   Epoch 55/150	 Time: 5.774	 Loss: 0.00369205
2025-05-21 15:51:21,157 - root - INFO -   Epoch 56/150	 Time: 5.771	 Loss: 0.00352743
2025-05-21 15:51:26,925 - root - INFO -   Epoch 57/150	 Time: 5.767	 Loss: 0.00364621
2025-05-21 15:51:32,694 - root - INFO -   Epoch 58/150	 Time: 5.769	 Loss: 0.00347863
2025-05-21 15:51:38,459 - root - INFO -   Epoch 59/150	 Time: 5.765	 Loss: 0.00352118
2025-05-21 15:51:44,226 - root - INFO -   Epoch 60/150	 Time: 5.767	 Loss: 0.00352011
2025-05-21 15:51:50,004 - root - INFO -   Epoch 61/150	 Time: 5.778	 Loss: 0.00345270
2025-05-21 15:51:55,791 - root - INFO -   Epoch 62/150	 Time: 5.786	 Loss: 0.00351095
2025-05-21 15:52:01,571 - root - INFO -   Epoch 63/150	 Time: 5.780	 Loss: 0.00354217
2025-05-21 15:52:07,353 - root - INFO -   Epoch 64/150	 Time: 5.782	 Loss: 0.00341485
2025-05-21 15:52:13,166 - root - INFO -   Epoch 65/150	 Time: 5.813	 Loss: 0.00340105
2025-05-21 15:52:19,010 - root - INFO -   Epoch 66/150	 Time: 5.844	 Loss: 0.00342463
2025-05-21 15:52:24,799 - root - INFO -   Epoch 67/150	 Time: 5.788	 Loss: 0.00337570
2025-05-21 15:52:30,586 - root - INFO -   Epoch 68/150	 Time: 5.787	 Loss: 0.00341764
2025-05-21 15:52:36,371 - root - INFO -   Epoch 69/150	 Time: 5.785	 Loss: 0.00326687
2025-05-21 15:52:42,139 - root - INFO -   Epoch 70/150	 Time: 5.769	 Loss: 0.00328726
2025-05-21 15:52:47,918 - root - INFO -   Epoch 71/150	 Time: 5.778	 Loss: 0.00324538
2025-05-21 15:52:53,707 - root - INFO -   Epoch 72/150	 Time: 5.790	 Loss: 0.00338432
2025-05-21 15:52:59,502 - root - INFO -   Epoch 73/150	 Time: 5.794	 Loss: 0.00317920
2025-05-21 15:53:05,280 - root - INFO -   Epoch 74/150	 Time: 5.779	 Loss: 0.00323952
2025-05-21 15:53:11,047 - root - INFO -   Epoch 75/150	 Time: 5.766	 Loss: 0.00323283
2025-05-21 15:53:16,813 - root - INFO -   Epoch 76/150	 Time: 5.766	 Loss: 0.00309597
2025-05-21 15:53:22,634 - root - INFO -   Epoch 77/150	 Time: 5.821	 Loss: 0.00319991
2025-05-21 15:53:28,429 - root - INFO -   Epoch 78/150	 Time: 5.794	 Loss: 0.00314158
2025-05-21 15:53:34,196 - root - INFO -   Epoch 79/150	 Time: 5.767	 Loss: 0.00310560
2025-05-21 15:53:39,960 - root - INFO -   Epoch 80/150	 Time: 5.765	 Loss: 0.00303167
2025-05-21 15:53:45,726 - root - INFO -   Epoch 81/150	 Time: 5.765	 Loss: 0.00307945
2025-05-21 15:53:51,502 - root - INFO -   Epoch 82/150	 Time: 5.776	 Loss: 0.00304001
2025-05-21 15:53:57,274 - root - INFO -   Epoch 83/150	 Time: 5.771	 Loss: 0.00303880
2025-05-21 15:54:03,040 - root - INFO -   Epoch 84/150	 Time: 5.766	 Loss: 0.00304437
2025-05-21 15:54:08,809 - root - INFO -   Epoch 85/150	 Time: 5.769	 Loss: 0.00306212
2025-05-21 15:54:14,581 - root - INFO -   Epoch 86/150	 Time: 5.772	 Loss: 0.00298447
2025-05-21 15:54:20,363 - root - INFO -   Epoch 87/150	 Time: 5.782	 Loss: 0.00300407
2025-05-21 15:54:26,156 - root - INFO -   Epoch 88/150	 Time: 5.793	 Loss: 0.00298824
2025-05-21 15:54:31,968 - root - INFO -   Epoch 89/150	 Time: 5.811	 Loss: 0.00299122
2025-05-21 15:54:37,744 - root - INFO -   Epoch 90/150	 Time: 5.776	 Loss: 0.00292255
2025-05-21 15:54:43,504 - root - INFO -   Epoch 91/150	 Time: 5.759	 Loss: 0.00296071
2025-05-21 15:54:49,265 - root - INFO -   Epoch 92/150	 Time: 5.761	 Loss: 0.00290446
2025-05-21 15:54:55,034 - root - INFO -   Epoch 93/150	 Time: 5.768	 Loss: 0.00292987
2025-05-21 15:55:00,800 - root - INFO -   Epoch 94/150	 Time: 5.766	 Loss: 0.00284987
2025-05-21 15:55:06,569 - root - INFO -   Epoch 95/150	 Time: 5.769	 Loss: 0.00288854
2025-05-21 15:55:12,338 - root - INFO -   Epoch 96/150	 Time: 5.769	 Loss: 0.00287540
2025-05-21 15:55:18,100 - root - INFO -   Epoch 97/150	 Time: 5.762	 Loss: 0.00278245
2025-05-21 15:55:23,869 - root - INFO -   Epoch 98/150	 Time: 5.768	 Loss: 0.00284217
2025-05-21 15:55:29,639 - root - INFO -   Epoch 99/150	 Time: 5.770	 Loss: 0.00280077
2025-05-21 15:55:35,437 - root - INFO -   Epoch 100/150	 Time: 5.797	 Loss: 0.00276108
2025-05-21 15:55:41,261 - root - INFO -   Epoch 101/150	 Time: 5.824	 Loss: 0.00278539
2025-05-21 15:55:47,031 - root - INFO -   Epoch 102/150	 Time: 5.769	 Loss: 0.00270963
2025-05-21 15:55:52,804 - root - INFO -   Epoch 103/150	 Time: 5.773	 Loss: 0.00273831
2025-05-21 15:55:58,562 - root - INFO -   Epoch 104/150	 Time: 5.757	 Loss: 0.00271336
2025-05-21 15:56:04,323 - root - INFO -   Epoch 105/150	 Time: 5.762	 Loss: 0.00268335
2025-05-21 15:56:10,081 - root - INFO -   Epoch 106/150	 Time: 5.758	 Loss: 0.00274370
2025-05-21 15:56:15,834 - root - INFO -   Epoch 107/150	 Time: 5.752	 Loss: 0.00267088
2025-05-21 15:56:21,610 - root - INFO -   Epoch 108/150	 Time: 5.776	 Loss: 0.00266147
2025-05-21 15:56:27,383 - root - INFO -   Epoch 109/150	 Time: 5.773	 Loss: 0.00257135
2025-05-21 15:56:33,167 - root - INFO -   Epoch 110/150	 Time: 5.783	 Loss: 0.00264110
2025-05-21 15:56:38,969 - root - INFO -   Epoch 111/150	 Time: 5.802	 Loss: 0.00261770
2025-05-21 15:56:44,779 - root - INFO -   Epoch 112/150	 Time: 5.809	 Loss: 0.00260761
2025-05-21 15:56:50,566 - root - INFO -   Epoch 113/150	 Time: 5.786	 Loss: 0.00252884
2025-05-21 15:56:56,410 - root - INFO -   Epoch 114/150	 Time: 5.844	 Loss: 0.00259995
2025-05-21 15:57:02,190 - root - INFO -   Epoch 115/150	 Time: 5.780	 Loss: 0.00254859
2025-05-21 15:57:07,964 - root - INFO -   Epoch 116/150	 Time: 5.774	 Loss: 0.00253183
2025-05-21 15:57:13,747 - root - INFO -   Epoch 117/150	 Time: 5.783	 Loss: 0.00250233
2025-05-21 15:57:19,544 - root - INFO -   Epoch 118/150	 Time: 5.797	 Loss: 0.00252309
2025-05-21 15:57:25,312 - root - INFO -   Epoch 119/150	 Time: 5.768	 Loss: 0.00246149
2025-05-21 15:57:31,080 - root - INFO -   Epoch 120/150	 Time: 5.767	 Loss: 0.00244917
2025-05-21 15:57:36,856 - root - INFO -   Epoch 121/150	 Time: 5.776	 Loss: 0.00246667
2025-05-21 15:57:42,640 - root - INFO -   Epoch 122/150	 Time: 5.784	 Loss: 0.00244973
2025-05-21 15:57:48,456 - root - INFO -   Epoch 123/150	 Time: 5.815	 Loss: 0.00240216
2025-05-21 15:57:54,289 - root - INFO -   Epoch 124/150	 Time: 5.833	 Loss: 0.00242670
2025-05-21 15:58:00,082 - root - INFO -   Epoch 125/150	 Time: 5.792	 Loss: 0.00246920
2025-05-21 15:58:05,863 - root - INFO -   Epoch 126/150	 Time: 5.781	 Loss: 0.00239664
2025-05-21 15:58:11,642 - root - INFO -   Epoch 127/150	 Time: 5.778	 Loss: 0.00239096
2025-05-21 15:58:17,433 - root - INFO -   Epoch 128/150	 Time: 5.791	 Loss: 0.00236298
2025-05-21 15:58:23,224 - root - INFO -   Epoch 129/150	 Time: 5.791	 Loss: 0.00233661
2025-05-21 15:58:29,011 - root - INFO -   Epoch 130/150	 Time: 5.786	 Loss: 0.00234827
2025-05-21 15:58:34,792 - root - INFO -   Epoch 131/150	 Time: 5.780	 Loss: 0.00235122
2025-05-21 15:58:40,577 - root - INFO -   Epoch 132/150	 Time: 5.785	 Loss: 0.00228714
2025-05-21 15:58:46,377 - root - INFO -   Epoch 133/150	 Time: 5.800	 Loss: 0.00227393
2025-05-21 15:58:52,181 - root - INFO -   Epoch 134/150	 Time: 5.803	 Loss: 0.00232235
2025-05-21 15:58:58,008 - root - INFO -   Epoch 135/150	 Time: 5.827	 Loss: 0.00225149
2025-05-21 15:59:03,851 - root - INFO -   Epoch 136/150	 Time: 5.843	 Loss: 0.00229453
2025-05-21 15:59:09,632 - root - INFO -   Epoch 137/150	 Time: 5.781	 Loss: 0.00229252
2025-05-21 15:59:15,414 - root - INFO -   Epoch 138/150	 Time: 5.781	 Loss: 0.00223966
2025-05-21 15:59:21,208 - root - INFO -   Epoch 139/150	 Time: 5.793	 Loss: 0.00224808
2025-05-21 15:59:26,999 - root - INFO -   Epoch 140/150	 Time: 5.791	 Loss: 0.00218498
2025-05-21 15:59:32,792 - root - INFO -   Epoch 141/150	 Time: 5.793	 Loss: 0.00221346
2025-05-21 15:59:38,576 - root - INFO -   Epoch 142/150	 Time: 5.784	 Loss: 0.00223936
2025-05-21 15:59:44,354 - root - INFO -   Epoch 143/150	 Time: 5.778	 Loss: 0.00227355
2025-05-21 15:59:50,151 - root - INFO -   Epoch 144/150	 Time: 5.797	 Loss: 0.00221430
2025-05-21 15:59:55,925 - root - INFO -   Epoch 145/150	 Time: 5.774	 Loss: 0.00216269
2025-05-21 16:00:01,727 - root - INFO -   Epoch 146/150	 Time: 5.801	 Loss: 0.00215895
2025-05-21 16:00:07,533 - root - INFO -   Epoch 147/150	 Time: 5.806	 Loss: 0.00217283
2025-05-21 16:00:13,322 - root - INFO -   Epoch 148/150	 Time: 5.788	 Loss: 0.00216850
2025-05-21 16:00:19,099 - root - INFO -   Epoch 149/150	 Time: 5.776	 Loss: 0.00212359
2025-05-21 16:00:24,881 - root - INFO -   Epoch 150/150	 Time: 5.782	 Loss: 0.00213616
2025-05-21 16:00:24,881 - root - INFO - Training time: 866.918
2025-05-21 16:00:24,881 - root - INFO - Finished training.
2025-05-21 16:00:24,882 - root - INFO - Starting testing...
2025-05-21 16:00:26,543 - root - INFO - Testing time: 1.661
2025-05-21 16:00:26,662 - root - INFO - Test set AUC: 99.97%
2025-05-21 16:00:26,664 - root - INFO - Testing TPR95: 0.9991
2025-05-21 16:00:26,664 - root - INFO - Finished testing.
