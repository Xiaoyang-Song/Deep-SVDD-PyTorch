2025-05-28 21:14:27,442 - root - INFO - Log file is ../log/mnist-fashionmnist-32/log.txt.
2025-05-28 21:14:27,442 - root - INFO - Data path is ../data.
2025-05-28 21:14:27,442 - root - INFO - Export path is ../log/mnist-fashionmnist-32.
2025-05-28 21:14:27,443 - root - INFO - Dataset: mnist-fashionmnist-32
2025-05-28 21:14:27,443 - root - INFO - Normal class: [0]
2025-05-28 21:14:27,443 - root - INFO - Conducting cross-dataset experiment...
2025-05-28 21:14:27,443 - root - INFO - Network: cifar10_LeNet
2025-05-28 21:14:27,443 - root - INFO - Deep SVDD objective: one-class
2025-05-28 21:14:27,443 - root - INFO - Nu-paramerter: 0.10
2025-05-28 21:14:27,491 - root - INFO - Computation device: cuda
2025-05-28 21:14:27,491 - root - INFO - Number of dataloader workers: 0
2025-05-28 21:14:47,005 - root - INFO - Log file is ../log/mnist-fashionmnist-32/log.txt.
2025-05-28 21:14:47,005 - root - INFO - Data path is ../data.
2025-05-28 21:14:47,005 - root - INFO - Export path is ../log/mnist-fashionmnist-32.
2025-05-28 21:14:47,005 - root - INFO - Dataset: mnist-fashionmnist-32
2025-05-28 21:14:47,005 - root - INFO - Normal class: [0]
2025-05-28 21:14:47,005 - root - INFO - Conducting cross-dataset experiment...
2025-05-28 21:14:47,005 - root - INFO - Network: cifar10_LeNet
2025-05-28 21:14:47,005 - root - INFO - Deep SVDD objective: one-class
2025-05-28 21:14:47,005 - root - INFO - Nu-paramerter: 0.10
2025-05-28 21:14:47,030 - root - INFO - Computation device: cuda
2025-05-28 21:14:47,031 - root - INFO - Number of dataloader workers: 0
2025-05-28 21:14:47,442 - root - INFO - Pretraining: True
2025-05-28 21:14:47,442 - root - INFO - Pretraining optimizer: adam
2025-05-28 21:14:47,442 - root - INFO - Pretraining learning rate: 0.0001
2025-05-28 21:14:47,442 - root - INFO - Pretraining epochs: 350
2025-05-28 21:14:47,442 - root - INFO - Pretraining learning rate scheduler milestones: (250,)
2025-05-28 21:14:47,442 - root - INFO - Pretraining batch size: 200
2025-05-28 21:14:47,442 - root - INFO - Pretraining weight decay: 5e-07
2025-05-28 21:14:51,825 - root - INFO - Starting pretraining...
2025-05-28 21:24:33,180 - root - INFO - Log file is ../log/mnist-fashionmnist-32/log.txt.
2025-05-28 21:24:33,181 - root - INFO - Data path is ../data.
2025-05-28 21:24:33,181 - root - INFO - Export path is ../log/mnist-fashionmnist-32.
2025-05-28 21:24:33,181 - root - INFO - Dataset: mnist-fashionmnist-32
2025-05-28 21:24:33,181 - root - INFO - Normal class: [0]
2025-05-28 21:24:33,181 - root - INFO - Conducting cross-dataset experiment...
2025-05-28 21:24:33,181 - root - INFO - Network: cifar10_LeNet
2025-05-28 21:24:33,181 - root - INFO - Deep SVDD objective: one-class
2025-05-28 21:24:33,181 - root - INFO - Nu-paramerter: 0.10
2025-05-28 21:24:33,208 - root - INFO - Computation device: cuda
2025-05-28 21:24:33,209 - root - INFO - Number of dataloader workers: 0
2025-05-28 21:24:33,466 - root - INFO - Pretraining: True
2025-05-28 21:24:33,466 - root - INFO - Pretraining optimizer: adam
2025-05-28 21:24:33,466 - root - INFO - Pretraining learning rate: 0.0001
2025-05-28 21:24:33,466 - root - INFO - Pretraining epochs: 350
2025-05-28 21:24:33,466 - root - INFO - Pretraining learning rate scheduler milestones: (250,)
2025-05-28 21:24:33,466 - root - INFO - Pretraining batch size: 200
2025-05-28 21:24:33,466 - root - INFO - Pretraining weight decay: 5e-07
2025-05-28 21:24:36,264 - root - INFO - Starting pretraining...
2025-05-28 21:34:34,106 - root - INFO - Log file is ../log/mnist-fashionmnist-32/log.txt.
2025-05-28 21:34:34,107 - root - INFO - Data path is ../data.
2025-05-28 21:34:34,107 - root - INFO - Export path is ../log/mnist-fashionmnist-32.
2025-05-28 21:34:34,107 - root - INFO - Dataset: mnist-fashionmnist-32
2025-05-28 21:34:34,107 - root - INFO - Normal class: [0]
2025-05-28 21:34:34,107 - root - INFO - Conducting cross-dataset experiment...
2025-05-28 21:34:34,107 - root - INFO - Network: cifar10_LeNet
2025-05-28 21:34:34,107 - root - INFO - Deep SVDD objective: one-class
2025-05-28 21:34:34,110 - root - INFO - Nu-paramerter: 0.10
2025-05-28 21:34:34,136 - root - INFO - Computation device: cuda
2025-05-28 21:34:34,136 - root - INFO - Number of dataloader workers: 0
2025-05-28 21:34:34,232 - root - INFO - Pretraining: True
2025-05-28 21:34:34,232 - root - INFO - Pretraining optimizer: adam
2025-05-28 21:34:34,232 - root - INFO - Pretraining learning rate: 0.0001
2025-05-28 21:34:34,232 - root - INFO - Pretraining epochs: 350
2025-05-28 21:34:34,232 - root - INFO - Pretraining learning rate scheduler milestones: (250,)
2025-05-28 21:34:34,232 - root - INFO - Pretraining batch size: 200
2025-05-28 21:34:34,232 - root - INFO - Pretraining weight decay: 5e-07
2025-05-28 21:34:37,015 - root - INFO - Starting pretraining...
2025-05-28 21:37:12,549 - root - INFO - Log file is ../log/mnist-fashionmnist-32/log.txt.
2025-05-28 21:37:12,549 - root - INFO - Data path is ../data.
2025-05-28 21:37:12,550 - root - INFO - Export path is ../log/mnist-fashionmnist-32.
2025-05-28 21:37:12,550 - root - INFO - Dataset: mnist-fashionmnist-32
2025-05-28 21:37:12,550 - root - INFO - Normal class: [0]
2025-05-28 21:37:12,550 - root - INFO - Conducting cross-dataset experiment...
2025-05-28 21:37:12,550 - root - INFO - Network: cifar10_LeNet
2025-05-28 21:37:12,550 - root - INFO - Deep SVDD objective: one-class
2025-05-28 21:37:12,550 - root - INFO - Nu-paramerter: 0.10
2025-05-28 21:37:12,578 - root - INFO - Computation device: cuda
2025-05-28 21:37:12,578 - root - INFO - Number of dataloader workers: 0
2025-05-28 21:40:26,742 - root - INFO - Log file is ../log/mnist-fashionmnist-32/log.txt.
2025-05-28 21:40:26,742 - root - INFO - Data path is ../data.
2025-05-28 21:40:26,742 - root - INFO - Export path is ../log/mnist-fashionmnist-32.
2025-05-28 21:40:26,742 - root - INFO - Dataset: mnist-fashionmnist-32
2025-05-28 21:40:26,742 - root - INFO - Normal class: [0]
2025-05-28 21:40:26,743 - root - INFO - Conducting cross-dataset experiment...
2025-05-28 21:40:26,743 - root - INFO - Network: cifar10_LeNet
2025-05-28 21:40:26,743 - root - INFO - Deep SVDD objective: one-class
2025-05-28 21:40:26,743 - root - INFO - Nu-paramerter: 0.10
2025-05-28 21:40:26,768 - root - INFO - Computation device: cuda
2025-05-28 21:40:26,768 - root - INFO - Number of dataloader workers: 0
2025-05-28 21:40:26,863 - root - INFO - Pretraining: True
2025-05-28 21:40:26,863 - root - INFO - Pretraining optimizer: adam
2025-05-28 21:40:26,863 - root - INFO - Pretraining learning rate: 0.0001
2025-05-28 21:40:26,863 - root - INFO - Pretraining epochs: 350
2025-05-28 21:40:26,863 - root - INFO - Pretraining learning rate scheduler milestones: (250,)
2025-05-28 21:40:26,863 - root - INFO - Pretraining batch size: 200
2025-05-28 21:40:26,863 - root - INFO - Pretraining weight decay: 5e-07
2025-05-28 21:40:29,662 - root - INFO - Starting pretraining...
2025-05-28 21:42:41,069 - root - INFO - Log file is ../log/mnist-fashionmnist-32/log.txt.
2025-05-28 21:42:41,069 - root - INFO - Data path is ../data.
2025-05-28 21:42:41,069 - root - INFO - Export path is ../log/mnist-fashionmnist-32.
2025-05-28 21:42:41,069 - root - INFO - Dataset: mnist-fashionmnist-32
2025-05-28 21:42:41,069 - root - INFO - Normal class: [0]
2025-05-28 21:42:41,069 - root - INFO - Conducting cross-dataset experiment...
2025-05-28 21:42:41,070 - root - INFO - Network: cifar10_LeNet
2025-05-28 21:42:41,070 - root - INFO - Deep SVDD objective: one-class
2025-05-28 21:42:41,070 - root - INFO - Nu-paramerter: 0.10
2025-05-28 21:42:41,098 - root - INFO - Computation device: cuda
2025-05-28 21:42:41,098 - root - INFO - Number of dataloader workers: 0
2025-05-28 21:42:41,193 - root - INFO - Pretraining: True
2025-05-28 21:42:41,194 - root - INFO - Pretraining optimizer: adam
2025-05-28 21:42:41,194 - root - INFO - Pretraining learning rate: 0.0001
2025-05-28 21:42:41,194 - root - INFO - Pretraining epochs: 350
2025-05-28 21:42:41,194 - root - INFO - Pretraining learning rate scheduler milestones: (250,)
2025-05-28 21:42:41,194 - root - INFO - Pretraining batch size: 200
2025-05-28 21:42:41,194 - root - INFO - Pretraining weight decay: 5e-07
2025-05-28 21:42:44,002 - root - INFO - Starting pretraining...
2025-05-28 21:42:58,807 - root - INFO -   Epoch 1/350	 Time: 14.801	 Loss: 175.16992284
2025-05-28 21:43:12,807 - root - INFO -   Epoch 2/350	 Time: 13.999	 Loss: 46.25949417
2025-05-28 21:43:26,766 - root - INFO -   Epoch 3/350	 Time: 13.959	 Loss: 31.05130295
2025-05-28 21:43:41,479 - root - INFO - Log file is ../log/mnist-fashionmnist-32/log.txt.
2025-05-28 21:43:41,479 - root - INFO - Data path is ../data.
2025-05-28 21:43:41,479 - root - INFO - Export path is ../log/mnist-fashionmnist-32.
2025-05-28 21:43:41,479 - root - INFO - Dataset: mnist-fashionmnist-32
2025-05-28 21:43:41,479 - root - INFO - Normal class: [0]
2025-05-28 21:43:41,480 - root - INFO - Conducting cross-dataset experiment...
2025-05-28 21:43:41,480 - root - INFO - Network: cifar10_LeNet
2025-05-28 21:43:41,480 - root - INFO - Deep SVDD objective: one-class
2025-05-28 21:43:41,480 - root - INFO - Nu-paramerter: 0.10
2025-05-28 21:43:41,507 - root - INFO - Computation device: cuda
2025-05-28 21:43:41,507 - root - INFO - Number of dataloader workers: 0
2025-05-28 21:43:41,603 - root - INFO - Pretraining: True
2025-05-28 21:43:41,603 - root - INFO - Pretraining optimizer: adam
2025-05-28 21:43:41,603 - root - INFO - Pretraining learning rate: 0.0001
2025-05-28 21:43:41,603 - root - INFO - Pretraining epochs: 350
2025-05-28 21:43:41,603 - root - INFO - Pretraining learning rate scheduler milestones: (250,)
2025-05-28 21:43:41,603 - root - INFO - Pretraining batch size: 200
2025-05-28 21:43:41,603 - root - INFO - Pretraining weight decay: 5e-07
2025-05-28 21:43:44,403 - root - INFO - Starting pretraining...
2025-05-28 22:32:15,260 - root - INFO - Log file is ../log/mnist-fashionmnist-32/log.txt.
2025-05-28 22:32:15,261 - root - INFO - Data path is ../data.
2025-05-28 22:32:15,261 - root - INFO - Export path is ../log/mnist-fashionmnist-32.
2025-05-28 22:32:15,261 - root - INFO - Dataset: mnist-fashionmnist-32
2025-05-28 22:32:15,261 - root - INFO - Normal class: [0]
2025-05-28 22:32:15,261 - root - INFO - Conducting cross-dataset experiment...
2025-05-28 22:32:15,261 - root - INFO - Network: cifar10_LeNet
2025-05-28 22:32:15,261 - root - INFO - Deep SVDD objective: one-class
2025-05-28 22:32:15,261 - root - INFO - Nu-paramerter: 0.10
2025-05-28 22:32:15,301 - root - INFO - Computation device: cuda
2025-05-28 22:32:15,301 - root - INFO - Number of dataloader workers: 0
2025-05-28 22:33:05,480 - root - INFO - Log file is ../log/mnist-fashionmnist-32/log.txt.
2025-05-28 22:33:05,481 - root - INFO - Data path is ../data.
2025-05-28 22:33:05,481 - root - INFO - Export path is ../log/mnist-fashionmnist-32.
2025-05-28 22:33:05,481 - root - INFO - Dataset: mnist-fashionmnist-32
2025-05-28 22:33:05,481 - root - INFO - Normal class: [0]
2025-05-28 22:33:05,481 - root - INFO - Conducting cross-dataset experiment...
2025-05-28 22:33:05,481 - root - INFO - Network: cifar10_LeNet
2025-05-28 22:33:05,481 - root - INFO - Deep SVDD objective: one-class
2025-05-28 22:33:05,481 - root - INFO - Nu-paramerter: 0.10
2025-05-28 22:33:05,506 - root - INFO - Computation device: cuda
2025-05-28 22:33:05,506 - root - INFO - Number of dataloader workers: 0
2025-05-28 22:33:37,205 - root - INFO - Log file is ../log/mnist-fashionmnist-32/log.txt.
2025-05-28 22:33:37,206 - root - INFO - Data path is ../data.
2025-05-28 22:33:37,206 - root - INFO - Export path is ../log/mnist-fashionmnist-32.
2025-05-28 22:33:37,206 - root - INFO - Dataset: mnist-fashionmnist-32
2025-05-28 22:33:37,206 - root - INFO - Normal class: [0]
2025-05-28 22:33:37,206 - root - INFO - Conducting cross-dataset experiment...
2025-05-28 22:33:37,206 - root - INFO - Network: cifar10_LeNet
2025-05-28 22:33:37,206 - root - INFO - Deep SVDD objective: one-class
2025-05-28 22:33:37,206 - root - INFO - Nu-paramerter: 0.10
2025-05-28 22:33:37,232 - root - INFO - Computation device: cuda
2025-05-28 22:33:37,232 - root - INFO - Number of dataloader workers: 0
2025-05-28 22:33:55,969 - root - INFO - Log file is ../log/mnist-fashionmnist-32/log.txt.
2025-05-28 22:33:55,970 - root - INFO - Data path is ../data.
2025-05-28 22:33:55,970 - root - INFO - Export path is ../log/mnist-fashionmnist-32.
2025-05-28 22:33:55,970 - root - INFO - Dataset: mnist-fashionmnist-32
2025-05-28 22:33:55,970 - root - INFO - Normal class: [0]
2025-05-28 22:33:55,970 - root - INFO - Conducting cross-dataset experiment...
2025-05-28 22:33:55,970 - root - INFO - Network: cifar10_LeNet
2025-05-28 22:33:55,970 - root - INFO - Deep SVDD objective: one-class
2025-05-28 22:33:55,971 - root - INFO - Nu-paramerter: 0.10
2025-05-28 22:33:56,008 - root - INFO - Computation device: cuda
2025-05-28 22:33:56,008 - root - INFO - Number of dataloader workers: 0
2025-05-28 22:34:11,990 - root - INFO - Pretraining: True
2025-05-28 22:34:11,990 - root - INFO - Pretraining optimizer: adam
2025-05-28 22:34:11,990 - root - INFO - Pretraining learning rate: 0.0001
2025-05-28 22:34:11,990 - root - INFO - Pretraining epochs: 350
2025-05-28 22:34:11,990 - root - INFO - Pretraining learning rate scheduler milestones: (250,)
2025-05-28 22:34:11,990 - root - INFO - Pretraining batch size: 200
2025-05-28 22:34:11,990 - root - INFO - Pretraining weight decay: 5e-07
2025-05-28 22:34:14,693 - root - INFO - Starting pretraining...
2025-05-28 22:34:51,977 - root - INFO - Log file is ../log/mnist-fashionmnist-32/log.txt.
2025-05-28 22:34:51,977 - root - INFO - Data path is ../data.
2025-05-28 22:34:51,977 - root - INFO - Export path is ../log/mnist-fashionmnist-32.
2025-05-28 22:34:51,977 - root - INFO - Dataset: mnist-fashionmnist-32
2025-05-28 22:34:51,977 - root - INFO - Normal class: [0]
2025-05-28 22:34:51,978 - root - INFO - Conducting cross-dataset experiment...
2025-05-28 22:34:51,978 - root - INFO - Network: cifar10_LeNet
2025-05-28 22:34:51,978 - root - INFO - Deep SVDD objective: one-class
2025-05-28 22:34:51,978 - root - INFO - Nu-paramerter: 0.10
2025-05-28 22:34:52,002 - root - INFO - Computation device: cuda
2025-05-28 22:34:52,003 - root - INFO - Number of dataloader workers: 0
2025-05-28 22:36:27,634 - root - INFO - Log file is ../log/mnist-fashionmnist-32/log.txt.
2025-05-28 22:36:27,635 - root - INFO - Data path is ../data.
2025-05-28 22:36:27,635 - root - INFO - Export path is ../log/mnist-fashionmnist-32.
2025-05-28 22:36:27,635 - root - INFO - Dataset: mnist-fashionmnist-32
2025-05-28 22:36:27,635 - root - INFO - Normal class: [0]
2025-05-28 22:36:27,635 - root - INFO - Conducting cross-dataset experiment...
2025-05-28 22:36:27,635 - root - INFO - Network: cifar10_LeNet
2025-05-28 22:36:27,635 - root - INFO - Deep SVDD objective: one-class
2025-05-28 22:36:27,635 - root - INFO - Nu-paramerter: 0.10
2025-05-28 22:36:27,675 - root - INFO - Computation device: cuda
2025-05-28 22:36:27,676 - root - INFO - Number of dataloader workers: 0
2025-05-28 22:36:43,766 - root - INFO - Pretraining: True
2025-05-28 22:36:43,767 - root - INFO - Pretraining optimizer: adam
2025-05-28 22:36:43,767 - root - INFO - Pretraining learning rate: 0.0001
2025-05-28 22:36:43,767 - root - INFO - Pretraining epochs: 350
2025-05-28 22:36:43,767 - root - INFO - Pretraining learning rate scheduler milestones: (250,)
2025-05-28 22:36:43,767 - root - INFO - Pretraining batch size: 200
2025-05-28 22:36:43,767 - root - INFO - Pretraining weight decay: 5e-07
2025-05-28 22:36:46,416 - root - INFO - Starting pretraining...
2025-05-28 22:38:45,301 - root - INFO - Log file is ../log/mnist-fashionmnist-32/log.txt.
2025-05-28 22:38:45,301 - root - INFO - Data path is ../data.
2025-05-28 22:38:45,301 - root - INFO - Export path is ../log/mnist-fashionmnist-32.
2025-05-28 22:38:45,301 - root - INFO - Dataset: mnist-fashionmnist-32
2025-05-28 22:38:45,302 - root - INFO - Normal class: [0]
2025-05-28 22:38:45,302 - root - INFO - Conducting cross-dataset experiment...
2025-05-28 22:38:45,302 - root - INFO - Network: cifar10_LeNet
2025-05-28 22:38:45,302 - root - INFO - Deep SVDD objective: one-class
2025-05-28 22:38:45,302 - root - INFO - Nu-paramerter: 0.10
2025-05-28 22:38:45,329 - root - INFO - Computation device: cuda
2025-05-28 22:38:45,329 - root - INFO - Number of dataloader workers: 0
2025-05-28 22:39:01,492 - root - INFO - Pretraining: True
2025-05-28 22:39:01,492 - root - INFO - Pretraining optimizer: adam
2025-05-28 22:39:01,492 - root - INFO - Pretraining learning rate: 0.0001
2025-05-28 22:39:01,492 - root - INFO - Pretraining epochs: 350
2025-05-28 22:39:01,492 - root - INFO - Pretraining learning rate scheduler milestones: (250,)
2025-05-28 22:39:01,492 - root - INFO - Pretraining batch size: 200
2025-05-28 22:39:01,492 - root - INFO - Pretraining weight decay: 5e-07
2025-05-28 22:39:04,126 - root - INFO - Starting pretraining...
2025-05-28 22:41:09,404 - root - INFO - Log file is ../log/mnist-fashionmnist-32/log.txt.
2025-05-28 22:41:09,405 - root - INFO - Data path is ../data.
2025-05-28 22:41:09,405 - root - INFO - Export path is ../log/mnist-fashionmnist-32.
2025-05-28 22:41:09,405 - root - INFO - Dataset: mnist-fashionmnist-32
2025-05-28 22:41:09,405 - root - INFO - Normal class: [0]
2025-05-28 22:41:09,405 - root - INFO - Conducting cross-dataset experiment...
2025-05-28 22:41:09,405 - root - INFO - Network: cifar10_LeNet
2025-05-28 22:41:09,405 - root - INFO - Deep SVDD objective: one-class
2025-05-28 22:41:09,405 - root - INFO - Nu-paramerter: 0.10
2025-05-28 22:41:09,430 - root - INFO - Computation device: cuda
2025-05-28 22:41:09,431 - root - INFO - Number of dataloader workers: 0
2025-05-28 22:41:25,318 - root - INFO - Pretraining: True
2025-05-28 22:41:25,318 - root - INFO - Pretraining optimizer: adam
2025-05-28 22:41:25,318 - root - INFO - Pretraining learning rate: 0.0001
2025-05-28 22:41:25,318 - root - INFO - Pretraining epochs: 350
2025-05-28 22:41:25,318 - root - INFO - Pretraining learning rate scheduler milestones: (250,)
2025-05-28 22:41:25,319 - root - INFO - Pretraining batch size: 200
2025-05-28 22:41:25,319 - root - INFO - Pretraining weight decay: 5e-07
2025-05-28 22:41:27,994 - root - INFO - Starting pretraining...
2025-05-28 22:51:37,928 - root - INFO - Log file is ../log/mnist-fashionmnist-32/log.txt.
2025-05-28 22:51:37,954 - root - INFO - Data path is ../data.
2025-05-28 22:51:37,954 - root - INFO - Export path is ../log/mnist-fashionmnist-32.
2025-05-28 22:51:37,954 - root - INFO - Dataset: mnist-fashionmnist-32
2025-05-28 22:51:37,954 - root - INFO - Normal class: [0]
2025-05-28 22:51:37,954 - root - INFO - Conducting cross-dataset experiment...
2025-05-28 22:51:37,954 - root - INFO - Network: cifar10_LeNet
2025-05-28 22:51:37,954 - root - INFO - Deep SVDD objective: one-class
2025-05-28 22:51:37,954 - root - INFO - Nu-paramerter: 0.10
2025-05-28 22:51:37,956 - root - INFO - Computation device: cpu
2025-05-28 22:51:37,957 - root - INFO - Number of dataloader workers: 0
2025-05-28 22:52:56,588 - root - INFO - Log file is ../log/mnist-fashionmnist-32/log.txt.
2025-05-28 22:52:56,600 - root - INFO - Data path is ../data.
2025-05-28 22:52:56,600 - root - INFO - Export path is ../log/mnist-fashionmnist-32.
2025-05-28 22:52:56,600 - root - INFO - Dataset: mnist-fashionmnist-32
2025-05-28 22:52:56,600 - root - INFO - Normal class: [0]
2025-05-28 22:52:56,600 - root - INFO - Conducting cross-dataset experiment...
2025-05-28 22:52:56,600 - root - INFO - Network: cifar10_LeNet
2025-05-28 22:52:56,600 - root - INFO - Deep SVDD objective: one-class
2025-05-28 22:52:56,600 - root - INFO - Nu-paramerter: 0.10
2025-05-28 22:52:56,601 - root - INFO - Computation device: cpu
2025-05-28 22:52:56,601 - root - INFO - Number of dataloader workers: 0
2025-05-28 22:53:19,804 - root - INFO - Pretraining: True
2025-05-28 22:53:19,804 - root - INFO - Pretraining optimizer: adam
2025-05-28 22:53:19,804 - root - INFO - Pretraining learning rate: 0.0001
2025-05-28 22:53:19,805 - root - INFO - Pretraining epochs: 350
2025-05-28 22:53:19,805 - root - INFO - Pretraining learning rate scheduler milestones: (250,)
2025-05-28 22:53:19,805 - root - INFO - Pretraining batch size: 200
2025-05-28 22:53:19,805 - root - INFO - Pretraining weight decay: 5e-07
2025-05-28 22:53:19,830 - root - INFO - Starting pretraining...
2025-05-28 22:57:22,153 - root - INFO - Log file is ../log/mnist-fashionmnist-32/log.txt.
2025-05-28 22:57:22,168 - root - INFO - Data path is ../data.
2025-05-28 22:57:22,168 - root - INFO - Export path is ../log/mnist-fashionmnist-32.
2025-05-28 22:57:22,168 - root - INFO - Dataset: mnist-fashionmnist-32
2025-05-28 22:57:22,169 - root - INFO - Normal class: [0]
2025-05-28 22:57:22,169 - root - INFO - Conducting cross-dataset experiment...
2025-05-28 22:57:22,169 - root - INFO - Network: cifar10_LeNet
2025-05-28 22:57:22,169 - root - INFO - Deep SVDD objective: one-class
2025-05-28 22:57:22,169 - root - INFO - Nu-paramerter: 0.10
2025-05-28 22:57:22,202 - root - INFO - Computation device: cuda
2025-05-28 22:57:22,202 - root - INFO - Number of dataloader workers: 0
2025-05-28 22:57:38,997 - root - INFO - Pretraining: True
2025-05-28 22:57:38,997 - root - INFO - Pretraining optimizer: adam
2025-05-28 22:57:38,997 - root - INFO - Pretraining learning rate: 0.0001
2025-05-28 22:57:38,997 - root - INFO - Pretraining epochs: 350
2025-05-28 22:57:38,997 - root - INFO - Pretraining learning rate scheduler milestones: (250,)
2025-05-28 22:57:38,997 - root - INFO - Pretraining batch size: 200
2025-05-28 22:57:38,997 - root - INFO - Pretraining weight decay: 5e-07
2025-05-28 22:57:41,651 - root - INFO - Starting pretraining...
2025-05-28 22:58:07,905 - root - INFO - Log file is ../log/mnist-fashionmnist-32/log.txt.
2025-05-28 22:58:07,906 - root - INFO - Data path is ../data.
2025-05-28 22:58:07,906 - root - INFO - Export path is ../log/mnist-fashionmnist-32.
2025-05-28 22:58:07,906 - root - INFO - Dataset: mnist-fashionmnist-32
2025-05-28 22:58:07,906 - root - INFO - Normal class: [0]
2025-05-28 22:58:07,906 - root - INFO - Conducting cross-dataset experiment...
2025-05-28 22:58:07,906 - root - INFO - Network: cifar10_LeNet
2025-05-28 22:58:07,906 - root - INFO - Deep SVDD objective: one-class
2025-05-28 22:58:07,906 - root - INFO - Nu-paramerter: 0.10
2025-05-28 22:58:07,933 - root - INFO - Computation device: cuda
2025-05-28 22:58:07,933 - root - INFO - Number of dataloader workers: 0
2025-05-28 22:58:24,360 - root - INFO - Pretraining: True
2025-05-28 22:58:24,361 - root - INFO - Pretraining optimizer: adam
2025-05-28 22:58:24,361 - root - INFO - Pretraining learning rate: 0.0001
2025-05-28 22:58:24,361 - root - INFO - Pretraining epochs: 350
2025-05-28 22:58:24,361 - root - INFO - Pretraining learning rate scheduler milestones: (250,)
2025-05-28 22:58:24,361 - root - INFO - Pretraining batch size: 200
2025-05-28 22:58:24,361 - root - INFO - Pretraining weight decay: 5e-07
2025-05-28 22:58:26,956 - root - INFO - Starting pretraining...
2025-05-28 22:58:36,827 - root - INFO -   Epoch 1/350	 Time: 9.871	 Loss: 170.40357506
2025-05-28 22:58:46,346 - root - INFO -   Epoch 2/350	 Time: 9.518	 Loss: 46.00808257
2025-05-28 23:11:21,435 - root - INFO - Log file is ../log/mnist-fashionmnist-32/log.txt.
2025-05-28 23:11:21,436 - root - INFO - Data path is ../data.
2025-05-28 23:11:21,436 - root - INFO - Export path is ../log/mnist-fashionmnist-32.
2025-05-28 23:11:21,436 - root - INFO - Dataset: mnist-fashionmnist-32
2025-05-28 23:11:21,436 - root - INFO - Normal class: [0]
2025-05-28 23:11:21,436 - root - INFO - Conducting cross-dataset experiment...
2025-05-28 23:11:21,436 - root - INFO - Network: cifar10_LeNet
2025-05-28 23:11:21,436 - root - INFO - Deep SVDD objective: one-class
2025-05-28 23:11:21,436 - root - INFO - Nu-paramerter: 0.10
2025-05-28 23:11:21,465 - root - INFO - Computation device: cuda
2025-05-28 23:11:21,465 - root - INFO - Number of dataloader workers: 0
2025-05-28 23:11:37,749 - root - INFO - Pretraining: True
2025-05-28 23:11:37,750 - root - INFO - Pretraining optimizer: adam
2025-05-28 23:11:37,750 - root - INFO - Pretraining learning rate: 0.0001
2025-05-28 23:11:37,750 - root - INFO - Pretraining epochs: 350
2025-05-28 23:11:37,750 - root - INFO - Pretraining learning rate scheduler milestones: (250,)
2025-05-28 23:11:37,750 - root - INFO - Pretraining batch size: 200
2025-05-28 23:11:37,750 - root - INFO - Pretraining weight decay: 5e-07
2025-05-28 23:11:40,487 - root - INFO - Starting pretraining...
2025-05-28 23:11:50,356 - root - INFO -   Epoch 1/350	 Time: 9.858	 Loss: 143.78921411
2025-05-28 23:11:59,866 - root - INFO -   Epoch 2/350	 Time: 9.509	 Loss: 39.01102170
2025-05-28 23:12:09,371 - root - INFO -   Epoch 3/350	 Time: 9.503	 Loss: 27.24615821
2025-05-28 23:12:18,956 - root - INFO -   Epoch 4/350	 Time: 9.584	 Loss: 22.13602161
2025-05-28 23:12:28,489 - root - INFO -   Epoch 5/350	 Time: 9.532	 Loss: 19.32857272
2025-05-28 23:12:37,986 - root - INFO -   Epoch 6/350	 Time: 9.496	 Loss: 17.43484149
2025-05-28 23:12:47,474 - root - INFO -   Epoch 7/350	 Time: 9.488	 Loss: 16.08965485
2025-05-28 23:12:57,113 - root - INFO -   Epoch 8/350	 Time: 9.638	 Loss: 15.11387108
2025-05-28 23:13:06,631 - root - INFO -   Epoch 9/350	 Time: 9.517	 Loss: 14.15628172
2025-05-28 23:13:16,130 - root - INFO -   Epoch 10/350	 Time: 9.500	 Loss: 13.56505727
2025-05-28 23:13:25,641 - root - INFO -   Epoch 11/350	 Time: 9.510	 Loss: 12.95674804
2025-05-28 23:13:35,229 - root - INFO -   Epoch 12/350	 Time: 9.588	 Loss: 12.52494405
2025-05-28 23:13:44,785 - root - INFO -   Epoch 13/350	 Time: 9.556	 Loss: 12.00531013
2025-05-28 23:13:54,281 - root - INFO -   Epoch 14/350	 Time: 9.495	 Loss: 11.66855081
2025-05-28 23:14:03,778 - root - INFO -   Epoch 15/350	 Time: 9.496	 Loss: 11.28735699
2025-05-28 23:14:13,271 - root - INFO -   Epoch 16/350	 Time: 9.493	 Loss: 10.95891148
2025-05-28 23:14:22,816 - root - INFO -   Epoch 17/350	 Time: 9.545	 Loss: 10.74817992
2025-05-28 23:14:32,393 - root - INFO -   Epoch 18/350	 Time: 9.577	 Loss: 10.37620346
2025-05-28 23:14:41,885 - root - INFO -   Epoch 19/350	 Time: 9.492	 Loss: 10.08988056
2025-05-28 23:14:51,380 - root - INFO -   Epoch 20/350	 Time: 9.495	 Loss: 10.02642817
2025-05-28 23:15:00,885 - root - INFO -   Epoch 21/350	 Time: 9.504	 Loss: 9.73933945
2025-05-28 23:15:10,400 - root - INFO -   Epoch 22/350	 Time: 9.514	 Loss: 9.60444979
2025-05-28 23:15:19,942 - root - INFO -   Epoch 23/350	 Time: 9.542	 Loss: 9.40547000
2025-05-28 23:15:29,546 - root - INFO -   Epoch 24/350	 Time: 9.603	 Loss: 9.26989733
2025-05-28 23:15:39,052 - root - INFO -   Epoch 25/350	 Time: 9.505	 Loss: 9.10745827
2025-05-28 23:15:48,566 - root - INFO -   Epoch 26/350	 Time: 9.514	 Loss: 8.94038788
2025-05-28 23:15:58,078 - root - INFO -   Epoch 27/350	 Time: 9.512	 Loss: 8.84199585
2025-05-28 23:16:07,610 - root - INFO -   Epoch 28/350	 Time: 9.532	 Loss: 8.78430015
2025-05-28 23:16:17,134 - root - INFO -   Epoch 29/350	 Time: 9.523	 Loss: 8.64060268
2025-05-28 23:16:26,744 - root - INFO -   Epoch 30/350	 Time: 9.610	 Loss: 8.48320887
2025-05-28 23:16:36,275 - root - INFO -   Epoch 31/350	 Time: 9.530	 Loss: 8.36987044
2025-05-28 23:16:45,788 - root - INFO -   Epoch 32/350	 Time: 9.513	 Loss: 8.25145387
2025-05-28 23:16:55,295 - root - INFO -   Epoch 33/350	 Time: 9.507	 Loss: 8.20816909
2025-05-28 23:17:04,802 - root - INFO -   Epoch 34/350	 Time: 9.506	 Loss: 8.14758571
2025-05-28 23:17:14,313 - root - INFO -   Epoch 35/350	 Time: 9.511	 Loss: 8.00534446
2025-05-28 23:17:23,860 - root - INFO -   Epoch 36/350	 Time: 9.546	 Loss: 7.88281453
2025-05-28 23:17:33,477 - root - INFO -   Epoch 37/350	 Time: 9.617	 Loss: 7.82023998
2025-05-28 23:17:42,988 - root - INFO -   Epoch 38/350	 Time: 9.511	 Loss: 7.82030930
2025-05-28 23:17:52,505 - root - INFO -   Epoch 39/350	 Time: 9.516	 Loss: 7.69195827
2025-05-28 23:18:02,009 - root - INFO -   Epoch 40/350	 Time: 9.504	 Loss: 7.60643309
2025-05-28 23:18:11,520 - root - INFO -   Epoch 41/350	 Time: 9.510	 Loss: 7.53023264
2025-05-28 23:18:21,038 - root - INFO -   Epoch 42/350	 Time: 9.518	 Loss: 7.39983487
2025-05-28 23:18:30,644 - root - INFO -   Epoch 43/350	 Time: 9.606	 Loss: 7.39553948
2025-05-28 23:18:40,190 - root - INFO -   Epoch 44/350	 Time: 9.545	 Loss: 7.36122645
2025-05-28 23:18:49,696 - root - INFO -   Epoch 45/350	 Time: 9.506	 Loss: 7.31571475
2025-05-28 23:18:59,201 - root - INFO -   Epoch 46/350	 Time: 9.504	 Loss: 7.24190429
2025-05-28 23:19:08,711 - root - INFO -   Epoch 47/350	 Time: 9.508	 Loss: 7.22021520
2025-05-28 23:19:18,231 - root - INFO -   Epoch 48/350	 Time: 9.519	 Loss: 7.08250321
2025-05-28 23:19:27,798 - root - INFO -   Epoch 49/350	 Time: 9.566	 Loss: 7.08959884
2025-05-28 23:19:37,441 - root - INFO -   Epoch 50/350	 Time: 9.643	 Loss: 7.01579991
2025-05-28 23:19:46,957 - root - INFO -   Epoch 51/350	 Time: 9.516	 Loss: 6.94923420
2025-05-28 23:19:56,480 - root - INFO -   Epoch 52/350	 Time: 9.523	 Loss: 6.85597810
2025-05-28 23:20:06,016 - root - INFO -   Epoch 53/350	 Time: 9.535	 Loss: 6.85894375
2025-05-28 23:20:15,533 - root - INFO -   Epoch 54/350	 Time: 9.517	 Loss: 6.83623641
2025-05-28 23:20:25,064 - root - INFO -   Epoch 55/350	 Time: 9.531	 Loss: 6.77846573
2025-05-28 23:20:34,694 - root - INFO -   Epoch 56/350	 Time: 9.629	 Loss: 6.67343372
2025-05-28 23:20:44,282 - root - INFO -   Epoch 57/350	 Time: 9.587	 Loss: 6.69807498
2025-05-28 23:20:53,824 - root - INFO -   Epoch 58/350	 Time: 9.543	 Loss: 6.68369957
2025-05-28 23:21:03,353 - root - INFO -   Epoch 59/350	 Time: 9.528	 Loss: 6.60041027
2025-05-28 23:21:12,867 - root - INFO -   Epoch 60/350	 Time: 9.514	 Loss: 6.55824461
2025-05-28 23:21:22,396 - root - INFO -   Epoch 61/350	 Time: 9.529	 Loss: 6.57546782
2025-05-28 23:21:31,931 - root - INFO -   Epoch 62/350	 Time: 9.535	 Loss: 6.48766475
2025-05-28 23:21:41,557 - root - INFO -   Epoch 63/350	 Time: 9.625	 Loss: 6.49023824
2025-05-28 23:21:51,077 - root - INFO -   Epoch 64/350	 Time: 9.520	 Loss: 6.42920119
2025-05-28 23:22:00,590 - root - INFO -   Epoch 65/350	 Time: 9.514	 Loss: 6.34748386
2025-05-28 23:22:10,110 - root - INFO -   Epoch 66/350	 Time: 9.519	 Loss: 6.36200719
2025-05-28 23:22:19,629 - root - INFO -   Epoch 67/350	 Time: 9.519	 Loss: 6.31192721
2025-05-28 23:22:29,152 - root - INFO -   Epoch 68/350	 Time: 9.522	 Loss: 6.27500380
2025-05-28 23:22:38,765 - root - INFO -   Epoch 69/350	 Time: 9.613	 Loss: 6.22415829
2025-05-28 23:22:48,353 - root - INFO -   Epoch 70/350	 Time: 9.588	 Loss: 6.22601357
2025-05-28 23:22:57,892 - root - INFO -   Epoch 71/350	 Time: 9.539	 Loss: 6.20970416
2025-05-28 23:23:07,420 - root - INFO -   Epoch 72/350	 Time: 9.528	 Loss: 6.13527854
2025-05-28 23:23:16,959 - root - INFO -   Epoch 73/350	 Time: 9.539	 Loss: 6.13789196
2025-05-28 23:23:26,504 - root - INFO -   Epoch 74/350	 Time: 9.544	 Loss: 6.07112547
2025-05-28 23:23:36,044 - root - INFO -   Epoch 75/350	 Time: 9.540	 Loss: 6.03724459
2025-05-28 23:23:45,669 - root - INFO -   Epoch 76/350	 Time: 9.625	 Loss: 6.02294731
2025-05-28 23:23:55,203 - root - INFO -   Epoch 77/350	 Time: 9.534	 Loss: 6.02900328
2025-05-28 23:24:04,733 - root - INFO -   Epoch 78/350	 Time: 9.529	 Loss: 6.01388591
2025-05-28 23:24:14,275 - root - INFO -   Epoch 79/350	 Time: 9.541	 Loss: 5.93511555
2025-05-28 23:24:23,806 - root - INFO -   Epoch 80/350	 Time: 9.531	 Loss: 5.92876665
2025-05-28 23:24:33,342 - root - INFO -   Epoch 81/350	 Time: 9.536	 Loss: 5.91707089
2025-05-28 23:24:42,963 - root - INFO -   Epoch 82/350	 Time: 9.621	 Loss: 5.86200787
2025-05-28 23:24:52,565 - root - INFO -   Epoch 83/350	 Time: 9.602	 Loss: 5.84832632
2025-05-28 23:25:02,120 - root - INFO -   Epoch 84/350	 Time: 9.555	 Loss: 5.83219288
2025-05-28 23:25:11,658 - root - INFO -   Epoch 85/350	 Time: 9.538	 Loss: 5.76460158
2025-05-28 23:25:21,200 - root - INFO -   Epoch 86/350	 Time: 9.541	 Loss: 5.80191228
2025-05-28 23:25:30,741 - root - INFO -   Epoch 87/350	 Time: 9.540	 Loss: 5.71885820
2025-05-28 23:25:40,297 - root - INFO -   Epoch 88/350	 Time: 9.556	 Loss: 5.74269268
2025-05-28 23:25:49,950 - root - INFO -   Epoch 89/350	 Time: 9.653	 Loss: 5.72223166
2025-05-28 23:25:59,501 - root - INFO -   Epoch 90/350	 Time: 9.551	 Loss: 5.68060763
2025-05-28 23:26:09,049 - root - INFO -   Epoch 91/350	 Time: 9.547	 Loss: 5.65049863
2025-05-28 23:26:18,596 - root - INFO -   Epoch 92/350	 Time: 9.547	 Loss: 5.65078365
2025-05-28 23:26:28,148 - root - INFO -   Epoch 93/350	 Time: 9.551	 Loss: 5.59879220
2025-05-28 23:26:37,716 - root - INFO -   Epoch 94/350	 Time: 9.568	 Loss: 5.57640507
2025-05-28 23:26:47,328 - root - INFO -   Epoch 95/350	 Time: 9.611	 Loss: 5.55357004
2025-05-28 23:26:56,919 - root - INFO -   Epoch 96/350	 Time: 9.591	 Loss: 5.52416493
2025-05-28 23:27:06,435 - root - INFO -   Epoch 97/350	 Time: 9.516	 Loss: 5.55220361
2025-05-28 23:27:15,947 - root - INFO -   Epoch 98/350	 Time: 9.511	 Loss: 5.55736360
2025-05-28 23:27:25,476 - root - INFO -   Epoch 99/350	 Time: 9.529	 Loss: 5.47308109
2025-05-28 23:27:35,016 - root - INFO -   Epoch 100/350	 Time: 9.540	 Loss: 5.41210424
2025-05-28 23:27:44,554 - root - INFO -   Epoch 101/350	 Time: 9.538	 Loss: 5.46347723
2025-05-28 23:27:54,194 - root - INFO -   Epoch 102/350	 Time: 9.640	 Loss: 5.43248409
2025-05-28 23:28:03,736 - root - INFO -   Epoch 103/350	 Time: 9.542	 Loss: 5.41935130
2025-05-28 23:28:13,261 - root - INFO -   Epoch 104/350	 Time: 9.524	 Loss: 5.36142555
2025-05-28 23:28:22,803 - root - INFO -   Epoch 105/350	 Time: 9.541	 Loss: 5.37326394
2025-05-28 23:28:32,328 - root - INFO -   Epoch 106/350	 Time: 9.525	 Loss: 5.34679266
2025-05-28 23:28:41,869 - root - INFO -   Epoch 107/350	 Time: 9.540	 Loss: 5.32098511
2025-05-28 23:28:51,470 - root - INFO -   Epoch 108/350	 Time: 9.601	 Loss: 5.26921094
2025-05-28 23:29:01,083 - root - INFO -   Epoch 109/350	 Time: 9.613	 Loss: 5.31360545
2025-05-28 23:29:10,622 - root - INFO -   Epoch 110/350	 Time: 9.538	 Loss: 5.31118406
2025-05-28 23:29:20,169 - root - INFO -   Epoch 111/350	 Time: 9.546	 Loss: 5.27397583
2025-05-28 23:29:29,713 - root - INFO -   Epoch 112/350	 Time: 9.544	 Loss: 5.23894070
2025-05-28 23:29:39,264 - root - INFO -   Epoch 113/350	 Time: 9.550	 Loss: 5.24379748
2025-05-28 23:29:48,813 - root - INFO -   Epoch 114/350	 Time: 9.549	 Loss: 5.21862437
2025-05-28 23:29:58,459 - root - INFO -   Epoch 115/350	 Time: 9.646	 Loss: 5.22942365
2025-05-28 23:30:08,033 - root - INFO -   Epoch 116/350	 Time: 9.574	 Loss: 5.16773890
2025-05-28 23:30:17,561 - root - INFO -   Epoch 117/350	 Time: 9.528	 Loss: 5.15705501
2025-05-28 23:30:27,098 - root - INFO -   Epoch 118/350	 Time: 9.537	 Loss: 5.14547226
2025-05-28 23:30:36,636 - root - INFO -   Epoch 119/350	 Time: 9.538	 Loss: 5.18993169
2025-05-28 23:30:46,176 - root - INFO -   Epoch 120/350	 Time: 9.540	 Loss: 5.17813519
2025-05-28 23:30:55,758 - root - INFO -   Epoch 121/350	 Time: 9.582	 Loss: 5.13377169
2025-05-28 23:31:05,368 - root - INFO -   Epoch 122/350	 Time: 9.609	 Loss: 5.12450698
2025-05-28 23:31:14,897 - root - INFO -   Epoch 123/350	 Time: 9.529	 Loss: 5.14239216
2025-05-28 23:31:24,433 - root - INFO -   Epoch 124/350	 Time: 9.535	 Loss: 5.07725468
2025-05-28 23:31:33,973 - root - INFO -   Epoch 125/350	 Time: 9.540	 Loss: 5.06343168
2025-05-28 23:31:43,524 - root - INFO -   Epoch 126/350	 Time: 9.551	 Loss: 5.00250829
2025-05-28 23:31:53,085 - root - INFO -   Epoch 127/350	 Time: 9.561	 Loss: 5.00812473
2025-05-28 23:32:02,747 - root - INFO -   Epoch 128/350	 Time: 9.661	 Loss: 5.00762807
2025-05-28 23:32:12,320 - root - INFO -   Epoch 129/350	 Time: 9.573	 Loss: 5.00127760
2025-05-28 23:32:21,859 - root - INFO -   Epoch 130/350	 Time: 9.539	 Loss: 4.96574249
2025-05-28 23:32:31,410 - root - INFO -   Epoch 131/350	 Time: 9.550	 Loss: 4.97786053
2025-05-28 23:32:40,974 - root - INFO -   Epoch 132/350	 Time: 9.564	 Loss: 4.94321808
2025-05-28 23:32:50,549 - root - INFO -   Epoch 133/350	 Time: 9.575	 Loss: 4.97308790
2025-05-28 23:33:00,170 - root - INFO -   Epoch 134/350	 Time: 9.620	 Loss: 4.90602319
2025-05-28 23:33:09,829 - root - INFO -   Epoch 135/350	 Time: 9.659	 Loss: 4.92837849
2025-05-28 23:33:19,394 - root - INFO -   Epoch 136/350	 Time: 9.565	 Loss: 4.95657606
2025-05-28 23:33:28,961 - root - INFO -   Epoch 137/350	 Time: 9.566	 Loss: 4.91541543
2025-05-28 23:33:38,515 - root - INFO -   Epoch 138/350	 Time: 9.554	 Loss: 4.90155507
2025-05-28 23:33:48,069 - root - INFO -   Epoch 139/350	 Time: 9.553	 Loss: 4.88190965
2025-05-28 23:33:57,643 - root - INFO -   Epoch 140/350	 Time: 9.574	 Loss: 4.87598147
2025-05-28 23:34:07,302 - root - INFO -   Epoch 141/350	 Time: 9.659	 Loss: 4.83793873
2025-05-28 23:34:16,867 - root - INFO -   Epoch 142/350	 Time: 9.565	 Loss: 4.85492084
2025-05-28 23:34:26,411 - root - INFO -   Epoch 143/350	 Time: 9.543	 Loss: 4.84816407
2025-05-28 23:34:35,954 - root - INFO -   Epoch 144/350	 Time: 9.543	 Loss: 4.82806814
2025-05-28 23:34:45,504 - root - INFO -   Epoch 145/350	 Time: 9.549	 Loss: 4.83140435
2025-05-28 23:34:55,064 - root - INFO -   Epoch 146/350	 Time: 9.560	 Loss: 4.83523198
2025-05-28 23:35:04,686 - root - INFO -   Epoch 147/350	 Time: 9.622	 Loss: 4.81069399
2025-05-28 23:35:14,350 - root - INFO -   Epoch 148/350	 Time: 9.664	 Loss: 4.76228880
2025-05-28 23:35:23,917 - root - INFO -   Epoch 149/350	 Time: 9.567	 Loss: 4.76238743
2025-05-28 23:35:33,478 - root - INFO -   Epoch 150/350	 Time: 9.561	 Loss: 4.77579634
2025-05-28 23:35:43,036 - root - INFO -   Epoch 151/350	 Time: 9.558	 Loss: 4.74266956
2025-05-28 23:35:52,608 - root - INFO -   Epoch 152/350	 Time: 9.572	 Loss: 4.73136712
2025-05-28 23:36:02,163 - root - INFO -   Epoch 153/350	 Time: 9.555	 Loss: 4.76072659
2025-05-28 23:36:11,836 - root - INFO -   Epoch 154/350	 Time: 9.673	 Loss: 4.72016009
2025-05-28 23:36:21,411 - root - INFO -   Epoch 155/350	 Time: 9.575	 Loss: 4.66811566
2025-05-28 23:36:30,961 - root - INFO -   Epoch 156/350	 Time: 9.549	 Loss: 4.70606229
2025-05-28 23:36:40,513 - root - INFO -   Epoch 157/350	 Time: 9.552	 Loss: 4.68579196
2025-05-28 23:36:50,077 - root - INFO -   Epoch 158/350	 Time: 9.564	 Loss: 4.66935732
2025-05-28 23:36:59,652 - root - INFO -   Epoch 159/350	 Time: 9.575	 Loss: 4.67932996
2025-05-28 23:37:09,284 - root - INFO -   Epoch 160/350	 Time: 9.631	 Loss: 4.64194376
2025-05-28 23:37:18,949 - root - INFO -   Epoch 161/350	 Time: 9.665	 Loss: 4.65466450
2025-05-28 23:37:28,511 - root - INFO -   Epoch 162/350	 Time: 9.561	 Loss: 4.61173870
2025-05-28 23:37:38,084 - root - INFO -   Epoch 163/350	 Time: 9.572	 Loss: 4.66412558
2025-05-28 23:37:47,656 - root - INFO -   Epoch 164/350	 Time: 9.572	 Loss: 4.58836640
2025-05-28 23:37:57,239 - root - INFO -   Epoch 165/350	 Time: 9.583	 Loss: 4.58949928
2025-05-28 23:38:06,832 - root - INFO -   Epoch 166/350	 Time: 9.592	 Loss: 4.63405397
2025-05-28 23:38:16,515 - root - INFO -   Epoch 167/350	 Time: 9.683	 Loss: 4.55637647
2025-05-28 23:38:26,110 - root - INFO -   Epoch 168/350	 Time: 9.595	 Loss: 4.56242535
2025-05-28 23:38:35,674 - root - INFO -   Epoch 169/350	 Time: 9.563	 Loss: 4.56683367
2025-05-28 23:38:45,235 - root - INFO -   Epoch 170/350	 Time: 9.561	 Loss: 4.58051885
2025-05-28 23:38:54,809 - root - INFO -   Epoch 171/350	 Time: 9.573	 Loss: 4.50956985
2025-05-28 23:39:04,389 - root - INFO -   Epoch 172/350	 Time: 9.581	 Loss: 4.55093424
2025-05-28 23:39:14,002 - root - INFO -   Epoch 173/350	 Time: 9.612	 Loss: 4.51396044
2025-05-28 23:39:23,672 - root - INFO -   Epoch 174/350	 Time: 9.670	 Loss: 4.56940319
2025-05-28 23:39:33,216 - root - INFO -   Epoch 175/350	 Time: 9.545	 Loss: 4.49264139
2025-05-28 23:39:42,762 - root - INFO -   Epoch 176/350	 Time: 9.545	 Loss: 4.49472348
2025-05-28 23:39:52,300 - root - INFO -   Epoch 177/350	 Time: 9.538	 Loss: 4.44203573
2025-05-28 23:40:01,871 - root - INFO -   Epoch 178/350	 Time: 9.570	 Loss: 4.44400453
2025-05-28 23:40:11,427 - root - INFO -   Epoch 179/350	 Time: 9.556	 Loss: 4.46333091
2025-05-28 23:40:21,088 - root - INFO -   Epoch 180/350	 Time: 9.660	 Loss: 4.39161405
2025-05-28 23:40:30,659 - root - INFO -   Epoch 181/350	 Time: 9.570	 Loss: 4.44039854
2025-05-28 23:40:40,202 - root - INFO -   Epoch 182/350	 Time: 9.542	 Loss: 4.43841024
2025-05-28 23:40:49,752 - root - INFO -   Epoch 183/350	 Time: 9.550	 Loss: 4.41086846
2025-05-28 23:40:59,313 - root - INFO -   Epoch 184/350	 Time: 9.561	 Loss: 4.41268818
2025-05-28 23:41:08,890 - root - INFO -   Epoch 185/350	 Time: 9.577	 Loss: 4.41090923
2025-05-28 23:41:18,476 - root - INFO -   Epoch 186/350	 Time: 9.585	 Loss: 4.43169898
2025-05-28 23:41:28,151 - root - INFO -   Epoch 187/350	 Time: 9.675	 Loss: 4.38345953
2025-05-28 23:41:37,710 - root - INFO -   Epoch 188/350	 Time: 9.559	 Loss: 4.36751361
2025-05-28 23:41:47,271 - root - INFO -   Epoch 189/350	 Time: 9.561	 Loss: 4.36732694
2025-05-28 23:41:56,851 - root - INFO -   Epoch 190/350	 Time: 9.580	 Loss: 4.40738566
2025-05-28 23:42:06,438 - root - INFO -   Epoch 191/350	 Time: 9.586	 Loss: 4.35378374
2025-05-28 23:42:16,019 - root - INFO -   Epoch 192/350	 Time: 9.581	 Loss: 4.32988182
2025-05-28 23:42:25,693 - root - INFO -   Epoch 193/350	 Time: 9.673	 Loss: 4.35025460
2025-05-28 23:42:35,275 - root - INFO -   Epoch 194/350	 Time: 9.582	 Loss: 4.35741243
2025-05-28 23:42:44,828 - root - INFO -   Epoch 195/350	 Time: 9.553	 Loss: 4.32885003
2025-05-28 23:42:54,390 - root - INFO -   Epoch 196/350	 Time: 9.562	 Loss: 4.33635224
2025-05-28 23:43:03,944 - root - INFO -   Epoch 197/350	 Time: 9.553	 Loss: 4.26193690
2025-05-28 23:43:13,530 - root - INFO -   Epoch 198/350	 Time: 9.586	 Loss: 4.30838091
2025-05-28 23:43:23,141 - root - INFO -   Epoch 199/350	 Time: 9.610	 Loss: 4.32334004
2025-05-28 23:43:32,810 - root - INFO -   Epoch 200/350	 Time: 9.668	 Loss: 4.33191432
2025-05-28 23:43:42,371 - root - INFO -   Epoch 201/350	 Time: 9.562	 Loss: 4.31554029
2025-05-28 23:43:51,938 - root - INFO -   Epoch 202/350	 Time: 9.567	 Loss: 4.27979583
2025-05-28 23:44:01,509 - root - INFO -   Epoch 203/350	 Time: 9.570	 Loss: 4.23480797
2025-05-28 23:44:11,104 - root - INFO -   Epoch 204/350	 Time: 9.595	 Loss: 4.26009446
2025-05-28 23:44:20,695 - root - INFO -   Epoch 205/350	 Time: 9.590	 Loss: 4.23880657
2025-05-28 23:44:30,387 - root - INFO -   Epoch 206/350	 Time: 9.692	 Loss: 4.24343018
2025-05-28 23:44:39,990 - root - INFO -   Epoch 207/350	 Time: 9.603	 Loss: 4.23142208
2025-05-28 23:44:49,567 - root - INFO -   Epoch 208/350	 Time: 9.577	 Loss: 4.20527702
2025-05-28 23:44:59,150 - root - INFO -   Epoch 209/350	 Time: 9.582	 Loss: 4.23011667
2025-05-28 23:45:08,726 - root - INFO -   Epoch 210/350	 Time: 9.575	 Loss: 4.22959613
2025-05-28 23:45:18,329 - root - INFO -   Epoch 211/350	 Time: 9.603	 Loss: 4.19940399
2025-05-28 23:45:27,941 - root - INFO -   Epoch 212/350	 Time: 9.612	 Loss: 4.19858392
2025-05-28 23:45:37,623 - root - INFO -   Epoch 213/350	 Time: 9.681	 Loss: 4.20261015
2025-05-28 23:45:47,216 - root - INFO -   Epoch 214/350	 Time: 9.593	 Loss: 4.19962362
2025-05-28 23:45:56,812 - root - INFO -   Epoch 215/350	 Time: 9.595	 Loss: 4.21591534
2025-05-28 23:46:06,408 - root - INFO -   Epoch 216/350	 Time: 9.596	 Loss: 4.18976840
2025-05-28 23:46:16,027 - root - INFO -   Epoch 217/350	 Time: 9.618	 Loss: 4.16787477
2025-05-28 23:46:25,614 - root - INFO -   Epoch 218/350	 Time: 9.586	 Loss: 4.15826755
2025-05-28 23:46:35,287 - root - INFO -   Epoch 219/350	 Time: 9.673	 Loss: 4.16418471
2025-05-28 23:46:44,878 - root - INFO -   Epoch 220/350	 Time: 9.590	 Loss: 4.11779706
2025-05-28 23:46:54,452 - root - INFO -   Epoch 221/350	 Time: 9.574	 Loss: 4.14470076
2025-05-28 23:47:04,017 - root - INFO -   Epoch 222/350	 Time: 9.565	 Loss: 4.13601566
2025-05-28 23:47:13,588 - root - INFO -   Epoch 223/350	 Time: 9.570	 Loss: 4.11604509
2025-05-28 23:47:23,171 - root - INFO -   Epoch 224/350	 Time: 9.583	 Loss: 4.10446698
2025-05-28 23:47:32,793 - root - INFO -   Epoch 225/350	 Time: 9.622	 Loss: 4.14191648
2025-05-28 23:47:42,447 - root - INFO -   Epoch 226/350	 Time: 9.654	 Loss: 4.12231043
2025-05-28 23:47:52,027 - root - INFO -   Epoch 227/350	 Time: 9.579	 Loss: 4.06964314
2025-05-28 23:48:01,638 - root - INFO -   Epoch 228/350	 Time: 9.611	 Loss: 4.06710167
2025-05-28 23:48:11,218 - root - INFO -   Epoch 229/350	 Time: 9.579	 Loss: 4.06119246
2025-05-28 23:48:20,812 - root - INFO -   Epoch 230/350	 Time: 9.593	 Loss: 4.08001190
2025-05-28 23:48:30,397 - root - INFO -   Epoch 231/350	 Time: 9.585	 Loss: 4.06969467
2025-05-28 23:48:40,097 - root - INFO -   Epoch 232/350	 Time: 9.700	 Loss: 4.05042966
2025-05-28 23:48:49,682 - root - INFO -   Epoch 233/350	 Time: 9.585	 Loss: 4.02431412
2025-05-28 23:48:59,250 - root - INFO -   Epoch 234/350	 Time: 9.568	 Loss: 4.06754914
2025-05-28 23:49:08,819 - root - INFO -   Epoch 235/350	 Time: 9.569	 Loss: 4.08456821
2025-05-28 23:49:18,392 - root - INFO -   Epoch 236/350	 Time: 9.573	 Loss: 4.01023871
2025-05-28 23:49:27,965 - root - INFO -   Epoch 237/350	 Time: 9.571	 Loss: 4.04097609
2025-05-28 23:49:37,565 - root - INFO -   Epoch 238/350	 Time: 9.600	 Loss: 4.05048654
2025-05-28 23:49:47,229 - root - INFO -   Epoch 239/350	 Time: 9.664	 Loss: 3.97682958
2025-05-28 23:49:56,816 - root - INFO -   Epoch 240/350	 Time: 9.586	 Loss: 4.04172138
2025-05-28 23:50:06,408 - root - INFO -   Epoch 241/350	 Time: 9.592	 Loss: 3.99886931
2025-05-28 23:50:15,987 - root - INFO -   Epoch 242/350	 Time: 9.579	 Loss: 4.00129229
2025-05-28 23:50:25,583 - root - INFO -   Epoch 243/350	 Time: 9.596	 Loss: 4.00575757
2025-05-28 23:50:35,162 - root - INFO -   Epoch 244/350	 Time: 9.578	 Loss: 3.98458405
2025-05-28 23:50:44,842 - root - INFO -   Epoch 245/350	 Time: 9.680	 Loss: 3.97013848
2025-05-28 23:50:54,452 - root - INFO -   Epoch 246/350	 Time: 9.610	 Loss: 4.00278205
2025-05-28 23:51:04,023 - root - INFO -   Epoch 247/350	 Time: 9.571	 Loss: 3.96910510
2025-05-28 23:51:13,601 - root - INFO -   Epoch 248/350	 Time: 9.578	 Loss: 3.96825491
2025-05-28 23:51:23,195 - root - INFO -   Epoch 249/350	 Time: 9.594	 Loss: 3.95336796
2025-05-28 23:51:32,788 - root - INFO -   Epoch 250/350	 Time: 9.592	 Loss: 3.68589192
2025-05-28 23:51:32,788 - root - INFO -   LR scheduler: new learning rate is 1e-05
2025-05-28 23:51:42,409 - root - INFO -   Epoch 251/350	 Time: 9.621	 Loss: 3.66215910
2025-05-28 23:51:52,067 - root - INFO -   Epoch 252/350	 Time: 9.658	 Loss: 3.66415863
2025-05-28 23:52:01,634 - root - INFO -   Epoch 253/350	 Time: 9.566	 Loss: 3.67415601
2025-05-28 23:52:11,211 - root - INFO -   Epoch 254/350	 Time: 9.577	 Loss: 3.66968109
2025-05-28 23:52:20,789 - root - INFO -   Epoch 255/350	 Time: 9.577	 Loss: 3.65798547
2025-05-28 23:52:30,361 - root - INFO -   Epoch 256/350	 Time: 9.572	 Loss: 3.63807278
2025-05-28 23:52:39,937 - root - INFO -   Epoch 257/350	 Time: 9.576	 Loss: 3.62624323
2025-05-28 23:52:49,626 - root - INFO -   Epoch 258/350	 Time: 9.688	 Loss: 3.65744835
2025-05-28 23:52:59,200 - root - INFO -   Epoch 259/350	 Time: 9.574	 Loss: 3.63701373
2025-05-28 23:53:08,761 - root - INFO -   Epoch 260/350	 Time: 9.560	 Loss: 3.63686356
2025-05-28 23:53:18,320 - root - INFO -   Epoch 261/350	 Time: 9.559	 Loss: 3.64650498
2025-05-28 23:53:27,879 - root - INFO -   Epoch 262/350	 Time: 9.558	 Loss: 3.63697988
2025-05-28 23:53:37,453 - root - INFO -   Epoch 263/350	 Time: 9.574	 Loss: 3.62823907
2025-05-28 23:53:47,055 - root - INFO -   Epoch 264/350	 Time: 9.602	 Loss: 3.66398650
2025-05-28 23:53:56,713 - root - INFO -   Epoch 265/350	 Time: 9.658	 Loss: 3.66281027
2025-05-28 23:54:06,283 - root - INFO -   Epoch 266/350	 Time: 9.569	 Loss: 3.64954682
2025-05-28 23:54:15,854 - root - INFO -   Epoch 267/350	 Time: 9.571	 Loss: 3.63508009
2025-05-28 23:54:25,446 - root - INFO -   Epoch 268/350	 Time: 9.592	 Loss: 3.66707443
2025-05-28 23:54:35,048 - root - INFO -   Epoch 269/350	 Time: 9.602	 Loss: 3.66711981
2025-05-28 23:54:44,635 - root - INFO -   Epoch 270/350	 Time: 9.587	 Loss: 3.62204832
2025-05-28 23:54:54,336 - root - INFO -   Epoch 271/350	 Time: 9.701	 Loss: 3.65567078
2025-05-28 23:55:03,957 - root - INFO -   Epoch 272/350	 Time: 9.620	 Loss: 3.62923009
2025-05-28 23:55:13,549 - root - INFO -   Epoch 273/350	 Time: 9.592	 Loss: 3.64048244
2025-05-28 23:55:23,129 - root - INFO -   Epoch 274/350	 Time: 9.580	 Loss: 3.62655825
2025-05-28 23:55:32,697 - root - INFO -   Epoch 275/350	 Time: 9.568	 Loss: 3.62938724
2025-05-28 23:55:42,298 - root - INFO -   Epoch 276/350	 Time: 9.600	 Loss: 3.64070045
2025-05-28 23:55:51,918 - root - INFO -   Epoch 277/350	 Time: 9.620	 Loss: 3.62293281
2025-05-28 23:56:01,581 - root - INFO -   Epoch 278/350	 Time: 9.663	 Loss: 3.63991937
2025-05-28 23:56:11,152 - root - INFO -   Epoch 279/350	 Time: 9.571	 Loss: 3.65066122
2025-05-28 23:56:20,718 - root - INFO -   Epoch 280/350	 Time: 9.566	 Loss: 3.62509839
2025-05-28 23:56:30,287 - root - INFO -   Epoch 281/350	 Time: 9.569	 Loss: 3.63851767
2025-05-28 23:56:39,874 - root - INFO -   Epoch 282/350	 Time: 9.586	 Loss: 3.62917849
2025-05-28 23:56:49,455 - root - INFO -   Epoch 283/350	 Time: 9.581	 Loss: 3.62898770
2025-05-28 23:56:59,139 - root - INFO -   Epoch 284/350	 Time: 9.683	 Loss: 3.65149249
2025-05-28 23:57:08,743 - root - INFO -   Epoch 285/350	 Time: 9.604	 Loss: 3.61942190
2025-05-28 23:57:18,314 - root - INFO -   Epoch 286/350	 Time: 9.571	 Loss: 3.61646465
2025-05-28 23:57:27,886 - root - INFO -   Epoch 287/350	 Time: 9.572	 Loss: 3.60595207
2025-05-28 23:57:37,468 - root - INFO -   Epoch 288/350	 Time: 9.582	 Loss: 3.61394088
2025-05-28 23:57:47,062 - root - INFO -   Epoch 289/350	 Time: 9.594	 Loss: 3.61725066
2025-05-28 23:57:56,707 - root - INFO -   Epoch 290/350	 Time: 9.645	 Loss: 3.60713418
2025-05-28 23:58:06,374 - root - INFO -   Epoch 291/350	 Time: 9.666	 Loss: 3.63490703
2025-05-28 23:58:15,968 - root - INFO -   Epoch 292/350	 Time: 9.594	 Loss: 3.61227398
2025-05-28 23:58:25,554 - root - INFO -   Epoch 293/350	 Time: 9.586	 Loss: 3.61702886
2025-05-28 23:58:35,167 - root - INFO -   Epoch 294/350	 Time: 9.613	 Loss: 3.62998534
2025-05-28 23:58:44,785 - root - INFO -   Epoch 295/350	 Time: 9.617	 Loss: 3.62062967
2025-05-28 23:58:54,400 - root - INFO -   Epoch 296/350	 Time: 9.615	 Loss: 3.61461650
2025-05-28 23:59:04,120 - root - INFO -   Epoch 297/350	 Time: 9.720	 Loss: 3.60524903
2025-05-28 23:59:13,751 - root - INFO -   Epoch 298/350	 Time: 9.631	 Loss: 3.62126274
2025-05-28 23:59:23,370 - root - INFO -   Epoch 299/350	 Time: 9.619	 Loss: 3.60074613
2025-05-28 23:59:32,956 - root - INFO -   Epoch 300/350	 Time: 9.586	 Loss: 3.61774950
2025-05-28 23:59:42,535 - root - INFO -   Epoch 301/350	 Time: 9.579	 Loss: 3.61809749
2025-05-28 23:59:52,136 - root - INFO -   Epoch 302/350	 Time: 9.600	 Loss: 3.61490656
2025-05-29 00:00:01,792 - root - INFO -   Epoch 303/350	 Time: 9.656	 Loss: 3.60163969
2025-05-29 00:00:11,464 - root - INFO -   Epoch 304/350	 Time: 9.672	 Loss: 3.61206040
2025-05-29 00:00:21,044 - root - INFO -   Epoch 305/350	 Time: 9.579	 Loss: 3.60323775
2025-05-29 00:00:30,621 - root - INFO -   Epoch 306/350	 Time: 9.577	 Loss: 3.59306762
2025-05-29 00:00:40,221 - root - INFO -   Epoch 307/350	 Time: 9.600	 Loss: 3.59919991
2025-05-29 00:00:49,826 - root - INFO -   Epoch 308/350	 Time: 9.605	 Loss: 3.60279741
2025-05-29 00:00:59,436 - root - INFO -   Epoch 309/350	 Time: 9.610	 Loss: 3.61004388
2025-05-29 00:01:09,149 - root - INFO -   Epoch 310/350	 Time: 9.713	 Loss: 3.60078072
2025-05-29 00:01:18,751 - root - INFO -   Epoch 311/350	 Time: 9.602	 Loss: 3.60760771
2025-05-29 00:01:28,336 - root - INFO -   Epoch 312/350	 Time: 9.585	 Loss: 3.58909873
2025-05-29 00:01:37,925 - root - INFO -   Epoch 313/350	 Time: 9.589	 Loss: 3.57448283
2025-05-29 00:01:47,510 - root - INFO -   Epoch 314/350	 Time: 9.584	 Loss: 3.59795972
2025-05-29 00:01:57,109 - root - INFO -   Epoch 315/350	 Time: 9.599	 Loss: 3.58290248
2025-05-29 00:02:06,757 - root - INFO -   Epoch 316/350	 Time: 9.648	 Loss: 3.59519451
2025-05-29 00:02:16,416 - root - INFO -   Epoch 317/350	 Time: 9.658	 Loss: 3.59586805
2025-05-29 00:02:26,005 - root - INFO -   Epoch 318/350	 Time: 9.589	 Loss: 3.58309708
2025-05-29 00:02:35,602 - root - INFO -   Epoch 319/350	 Time: 9.596	 Loss: 3.59362788
2025-05-29 00:02:45,191 - root - INFO -   Epoch 320/350	 Time: 9.589	 Loss: 3.59429581
2025-05-29 00:02:54,835 - root - INFO -   Epoch 321/350	 Time: 9.644	 Loss: 3.59697979
2025-05-29 00:03:04,448 - root - INFO -   Epoch 322/350	 Time: 9.612	 Loss: 3.60878000
2025-05-29 00:03:14,158 - root - INFO -   Epoch 323/350	 Time: 9.710	 Loss: 3.58482192
2025-05-29 00:03:23,777 - root - INFO -   Epoch 324/350	 Time: 9.619	 Loss: 3.59894839
2025-05-29 00:03:33,379 - root - INFO -   Epoch 325/350	 Time: 9.601	 Loss: 3.58506102
2025-05-29 00:03:42,990 - root - INFO -   Epoch 326/350	 Time: 9.611	 Loss: 3.58577689
2025-05-29 00:03:52,590 - root - INFO -   Epoch 327/350	 Time: 9.600	 Loss: 3.55835411
2025-05-29 00:04:02,220 - root - INFO -   Epoch 328/350	 Time: 9.629	 Loss: 3.58236843
2025-05-29 00:04:11,898 - root - INFO -   Epoch 329/350	 Time: 9.678	 Loss: 3.59453280
2025-05-29 00:04:21,551 - root - INFO -   Epoch 330/350	 Time: 9.652	 Loss: 3.58699056
2025-05-29 00:04:31,149 - root - INFO -   Epoch 331/350	 Time: 9.597	 Loss: 3.58252103
2025-05-29 00:04:40,750 - root - INFO -   Epoch 332/350	 Time: 9.601	 Loss: 3.59608909
2025-05-29 00:04:50,363 - root - INFO -   Epoch 333/350	 Time: 9.613	 Loss: 3.58659496
2025-05-29 00:05:00,001 - root - INFO -   Epoch 334/350	 Time: 9.638	 Loss: 3.56488084
2025-05-29 00:05:09,630 - root - INFO -   Epoch 335/350	 Time: 9.629	 Loss: 3.55815238
2025-05-29 00:05:19,357 - root - INFO -   Epoch 336/350	 Time: 9.726	 Loss: 3.59909122
2025-05-29 00:05:28,968 - root - INFO -   Epoch 337/350	 Time: 9.611	 Loss: 3.57768973
2025-05-29 00:05:38,563 - root - INFO -   Epoch 338/350	 Time: 9.595	 Loss: 3.57776567
2025-05-29 00:05:48,156 - root - INFO -   Epoch 339/350	 Time: 9.593	 Loss: 3.60224208
2025-05-29 00:05:57,748 - root - INFO -   Epoch 340/350	 Time: 9.592	 Loss: 3.57417469
2025-05-29 00:06:07,364 - root - INFO -   Epoch 341/350	 Time: 9.616	 Loss: 3.58389409
2025-05-29 00:06:17,026 - root - INFO -   Epoch 342/350	 Time: 9.662	 Loss: 3.55466285
2025-05-29 00:06:26,678 - root - INFO -   Epoch 343/350	 Time: 9.651	 Loss: 3.56006918
2025-05-29 00:06:36,267 - root - INFO -   Epoch 344/350	 Time: 9.589	 Loss: 3.55576221
2025-05-29 00:06:45,857 - root - INFO -   Epoch 345/350	 Time: 9.590	 Loss: 3.56931704
2025-05-29 00:06:55,445 - root - INFO -   Epoch 346/350	 Time: 9.588	 Loss: 3.56273125
2025-05-29 00:07:05,077 - root - INFO -   Epoch 347/350	 Time: 9.631	 Loss: 3.57235194
2025-05-29 00:07:14,694 - root - INFO -   Epoch 348/350	 Time: 9.617	 Loss: 3.57084490
2025-05-29 00:07:24,414 - root - INFO -   Epoch 349/350	 Time: 9.720	 Loss: 3.56178918
2025-05-29 00:07:34,024 - root - INFO -   Epoch 350/350	 Time: 9.610	 Loss: 3.57833012
2025-05-29 00:07:34,024 - root - INFO - Pretraining time: 3353.527
2025-05-29 00:07:34,024 - root - INFO - Finished pretraining.
2025-05-29 00:07:34,025 - root - INFO - Testing autoencoder...
2025-05-29 00:07:36,522 - root - INFO - Test set Loss: 93.30767344
2025-05-29 00:07:36,535 - root - INFO - Test set AUC: 100.00%
2025-05-29 00:07:36,535 - root - INFO - Autoencoder testing time: 2.510
2025-05-29 00:07:36,535 - root - INFO - Finished testing autoencoder.
2025-05-29 00:07:36,538 - root - INFO - Training optimizer: adam
2025-05-29 00:07:36,538 - root - INFO - Training learning rate: 0.0001
2025-05-29 00:07:36,538 - root - INFO - Training epochs: 150
2025-05-29 00:07:36,538 - root - INFO - Training learning rate scheduler milestones: (50,)
2025-05-29 00:07:36,538 - root - INFO - Training batch size: 200
2025-05-29 00:07:36,538 - root - INFO - Training weight decay: 5e-07
2025-05-29 00:07:36,539 - root - INFO - Initializing center c...
2025-05-29 00:07:43,047 - root - INFO - Center c initialized.
2025-05-29 00:07:43,047 - root - INFO - Starting training...
2025-05-29 00:07:51,015 - root - INFO -   Epoch 1/150	 Time: 7.968	 Loss: 22.63081408
2025-05-29 00:07:58,964 - root - INFO -   Epoch 2/150	 Time: 7.950	 Loss: 1.88686251
2025-05-29 00:08:06,956 - root - INFO -   Epoch 3/150	 Time: 7.991	 Loss: 1.05568103
2025-05-29 00:08:14,905 - root - INFO -   Epoch 4/150	 Time: 7.949	 Loss: 0.71886375
2025-05-29 00:08:22,945 - root - INFO -   Epoch 5/150	 Time: 8.039	 Loss: 0.55365825
2025-05-29 00:08:30,899 - root - INFO -   Epoch 6/150	 Time: 7.954	 Loss: 0.43445545
2025-05-29 00:08:38,837 - root - INFO -   Epoch 7/150	 Time: 7.938	 Loss: 0.36396633
2025-05-29 00:08:46,760 - root - INFO -   Epoch 8/150	 Time: 7.923	 Loss: 0.31473804
2025-05-29 00:08:54,691 - root - INFO -   Epoch 9/150	 Time: 7.931	 Loss: 0.27999355
2025-05-29 00:09:02,614 - root - INFO -   Epoch 10/150	 Time: 7.923	 Loss: 0.22890494
2025-05-29 00:09:10,578 - root - INFO -   Epoch 11/150	 Time: 7.964	 Loss: 0.22593694
2025-05-29 00:09:18,523 - root - INFO -   Epoch 12/150	 Time: 7.945	 Loss: 0.19265043
2025-05-29 00:09:26,519 - root - INFO -   Epoch 13/150	 Time: 7.995	 Loss: 0.16994781
2025-05-29 00:09:34,505 - root - INFO -   Epoch 14/150	 Time: 7.985	 Loss: 0.15777815
2025-05-29 00:09:42,423 - root - INFO -   Epoch 15/150	 Time: 7.918	 Loss: 0.14744217
2025-05-29 00:09:50,350 - root - INFO -   Epoch 16/150	 Time: 7.927	 Loss: 0.13689262
2025-05-29 00:09:58,281 - root - INFO -   Epoch 17/150	 Time: 7.931	 Loss: 0.11601578
2025-05-29 00:10:06,227 - root - INFO -   Epoch 18/150	 Time: 7.946	 Loss: 0.12645046
2025-05-29 00:10:14,208 - root - INFO -   Epoch 19/150	 Time: 7.981	 Loss: 0.11210755
2025-05-29 00:10:22,169 - root - INFO -   Epoch 20/150	 Time: 7.961	 Loss: 0.09885083
2025-05-29 00:10:30,215 - root - INFO -   Epoch 21/150	 Time: 8.045	 Loss: 0.08789935
2025-05-29 00:10:38,196 - root - INFO -   Epoch 22/150	 Time: 7.981	 Loss: 0.09457271
2025-05-29 00:10:46,141 - root - INFO -   Epoch 23/150	 Time: 7.945	 Loss: 0.07795627
2025-05-29 00:10:54,092 - root - INFO -   Epoch 24/150	 Time: 7.951	 Loss: 0.08024242
2025-05-29 00:11:02,044 - root - INFO -   Epoch 25/150	 Time: 7.951	 Loss: 0.08226984
2025-05-29 00:11:09,997 - root - INFO -   Epoch 26/150	 Time: 7.953	 Loss: 0.06421645
2025-05-29 00:11:17,982 - root - INFO -   Epoch 27/150	 Time: 7.985	 Loss: 0.06819346
2025-05-29 00:11:25,962 - root - INFO -   Epoch 28/150	 Time: 7.979	 Loss: 0.06025644
2025-05-29 00:11:34,030 - root - INFO -   Epoch 29/150	 Time: 8.068	 Loss: 0.06334173
2025-05-29 00:11:41,988 - root - INFO -   Epoch 30/150	 Time: 7.957	 Loss: 0.05910557
2025-05-29 00:11:49,924 - root - INFO -   Epoch 31/150	 Time: 7.937	 Loss: 0.05492738
2025-05-29 00:11:57,855 - root - INFO -   Epoch 32/150	 Time: 7.931	 Loss: 0.05262265
2025-05-29 00:12:05,803 - root - INFO -   Epoch 33/150	 Time: 7.948	 Loss: 0.04369078
2025-05-29 00:12:13,740 - root - INFO -   Epoch 34/150	 Time: 7.936	 Loss: 0.04366634
2025-05-29 00:12:21,713 - root - INFO -   Epoch 35/150	 Time: 7.973	 Loss: 0.04365489
2025-05-29 00:12:29,688 - root - INFO -   Epoch 36/150	 Time: 7.974	 Loss: 0.03965938
2025-05-29 00:12:37,721 - root - INFO -   Epoch 37/150	 Time: 8.033	 Loss: 0.03924629
2025-05-29 00:12:45,644 - root - INFO -   Epoch 38/150	 Time: 7.923	 Loss: 0.03764953
2025-05-29 00:12:53,578 - root - INFO -   Epoch 39/150	 Time: 7.934	 Loss: 0.03798012
2025-05-29 00:13:01,512 - root - INFO -   Epoch 40/150	 Time: 7.934	 Loss: 0.03261717
2025-05-29 00:13:09,454 - root - INFO -   Epoch 41/150	 Time: 7.941	 Loss: 0.03282262
2025-05-29 00:13:17,420 - root - INFO -   Epoch 42/150	 Time: 7.966	 Loss: 0.03313626
2025-05-29 00:13:25,382 - root - INFO -   Epoch 43/150	 Time: 7.962	 Loss: 0.03090071
2025-05-29 00:13:33,410 - root - INFO -   Epoch 44/150	 Time: 8.027	 Loss: 0.02985016
2025-05-29 00:13:41,414 - root - INFO -   Epoch 45/150	 Time: 8.004	 Loss: 0.03280638
2025-05-29 00:13:49,377 - root - INFO -   Epoch 46/150	 Time: 7.963	 Loss: 0.03029061
2025-05-29 00:13:57,342 - root - INFO -   Epoch 47/150	 Time: 7.965	 Loss: 0.02513423
2025-05-29 00:14:05,301 - root - INFO -   Epoch 48/150	 Time: 7.959	 Loss: 0.02412460
2025-05-29 00:14:13,277 - root - INFO -   Epoch 49/150	 Time: 7.975	 Loss: 0.02616991
2025-05-29 00:14:21,276 - root - INFO -   Epoch 50/150	 Time: 7.999	 Loss: 0.01410693
2025-05-29 00:14:21,277 - root - INFO -   LR scheduler: new learning rate is 1e-05
2025-05-29 00:14:29,243 - root - INFO -   Epoch 51/150	 Time: 7.966	 Loss: 0.01334682
2025-05-29 00:14:37,318 - root - INFO -   Epoch 52/150	 Time: 8.074	 Loss: 0.01406067
2025-05-29 00:14:45,300 - root - INFO -   Epoch 53/150	 Time: 7.982	 Loss: 0.01338071
2025-05-29 00:14:53,275 - root - INFO -   Epoch 54/150	 Time: 7.975	 Loss: 0.01342959
2025-05-29 00:15:01,255 - root - INFO -   Epoch 55/150	 Time: 7.980	 Loss: 0.01299399
2025-05-29 00:15:09,227 - root - INFO -   Epoch 56/150	 Time: 7.972	 Loss: 0.01407554
2025-05-29 00:15:17,184 - root - INFO -   Epoch 57/150	 Time: 7.957	 Loss: 0.01339319
2025-05-29 00:15:25,187 - root - INFO -   Epoch 58/150	 Time: 8.003	 Loss: 0.01302700
2025-05-29 00:15:33,171 - root - INFO -   Epoch 59/150	 Time: 7.983	 Loss: 0.01278688
2025-05-29 00:15:41,253 - root - INFO -   Epoch 60/150	 Time: 8.082	 Loss: 0.01295578
2025-05-29 00:15:49,226 - root - INFO -   Epoch 61/150	 Time: 7.973	 Loss: 0.01325187
2025-05-29 00:15:57,166 - root - INFO -   Epoch 62/150	 Time: 7.940	 Loss: 0.01308462
2025-05-29 00:16:05,107 - root - INFO -   Epoch 63/150	 Time: 7.941	 Loss: 0.01296956
2025-05-29 00:16:13,051 - root - INFO -   Epoch 64/150	 Time: 7.944	 Loss: 0.01337028
2025-05-29 00:16:20,989 - root - INFO -   Epoch 65/150	 Time: 7.937	 Loss: 0.01262663
2025-05-29 00:16:28,966 - root - INFO -   Epoch 66/150	 Time: 7.977	 Loss: 0.01292722
2025-05-29 00:16:36,966 - root - INFO -   Epoch 67/150	 Time: 8.000	 Loss: 0.01324079
2025-05-29 00:16:45,005 - root - INFO -   Epoch 68/150	 Time: 8.039	 Loss: 0.01300491
2025-05-29 00:16:52,970 - root - INFO -   Epoch 69/150	 Time: 7.965	 Loss: 0.01234936
2025-05-29 00:17:00,923 - root - INFO -   Epoch 70/150	 Time: 7.953	 Loss: 0.01252224
2025-05-29 00:17:08,892 - root - INFO -   Epoch 71/150	 Time: 7.968	 Loss: 0.01194116
2025-05-29 00:17:16,853 - root - INFO -   Epoch 72/150	 Time: 7.961	 Loss: 0.01193766
2025-05-29 00:17:24,856 - root - INFO -   Epoch 73/150	 Time: 8.002	 Loss: 0.01188900
2025-05-29 00:17:32,829 - root - INFO -   Epoch 74/150	 Time: 7.973	 Loss: 0.01223010
2025-05-29 00:17:40,883 - root - INFO -   Epoch 75/150	 Time: 8.054	 Loss: 0.01226932
2025-05-29 00:17:48,903 - root - INFO -   Epoch 76/150	 Time: 8.020	 Loss: 0.01212219
2025-05-29 00:17:56,858 - root - INFO -   Epoch 77/150	 Time: 7.955	 Loss: 0.01174459
2025-05-29 00:18:04,812 - root - INFO -   Epoch 78/150	 Time: 7.953	 Loss: 0.01143167
2025-05-29 00:18:12,744 - root - INFO -   Epoch 79/150	 Time: 7.933	 Loss: 0.01171378
2025-05-29 00:18:20,679 - root - INFO -   Epoch 80/150	 Time: 7.934	 Loss: 0.01145340
2025-05-29 00:18:28,633 - root - INFO -   Epoch 81/150	 Time: 7.954	 Loss: 0.01204026
2025-05-29 00:18:36,594 - root - INFO -   Epoch 82/150	 Time: 7.960	 Loss: 0.01093929
2025-05-29 00:18:44,638 - root - INFO -   Epoch 83/150	 Time: 8.044	 Loss: 0.01123959
2025-05-29 00:18:52,587 - root - INFO -   Epoch 84/150	 Time: 7.949	 Loss: 0.01171523
2025-05-29 00:19:00,527 - root - INFO -   Epoch 85/150	 Time: 7.939	 Loss: 0.01091802
2025-05-29 00:19:08,466 - root - INFO -   Epoch 86/150	 Time: 7.939	 Loss: 0.01154042
2025-05-29 00:19:16,405 - root - INFO -   Epoch 87/150	 Time: 7.938	 Loss: 0.01127050
2025-05-29 00:19:24,351 - root - INFO -   Epoch 88/150	 Time: 7.946	 Loss: 0.01049213
2025-05-29 00:19:32,336 - root - INFO -   Epoch 89/150	 Time: 7.984	 Loss: 0.01064260
2025-05-29 00:19:40,294 - root - INFO -   Epoch 90/150	 Time: 7.958	 Loss: 0.01152160
2025-05-29 00:19:48,354 - root - INFO -   Epoch 91/150	 Time: 8.060	 Loss: 0.01065705
2025-05-29 00:19:56,310 - root - INFO -   Epoch 92/150	 Time: 7.955	 Loss: 0.01057327
2025-05-29 00:20:04,317 - root - INFO -   Epoch 93/150	 Time: 8.007	 Loss: 0.01066316
2025-05-29 00:20:12,291 - root - INFO -   Epoch 94/150	 Time: 7.974	 Loss: 0.01112246
2025-05-29 00:20:20,256 - root - INFO -   Epoch 95/150	 Time: 7.965	 Loss: 0.01022954
2025-05-29 00:20:28,210 - root - INFO -   Epoch 96/150	 Time: 7.954	 Loss: 0.01059974
2025-05-29 00:20:36,206 - root - INFO -   Epoch 97/150	 Time: 7.996	 Loss: 0.01050491
2025-05-29 00:20:44,216 - root - INFO -   Epoch 98/150	 Time: 8.009	 Loss: 0.01003363
2025-05-29 00:20:52,246 - root - INFO -   Epoch 99/150	 Time: 8.030	 Loss: 0.01045848
2025-05-29 00:21:00,200 - root - INFO -   Epoch 100/150	 Time: 7.954	 Loss: 0.01005245
2025-05-29 00:21:08,167 - root - INFO -   Epoch 101/150	 Time: 7.966	 Loss: 0.00981206
2025-05-29 00:21:16,120 - root - INFO -   Epoch 102/150	 Time: 7.953	 Loss: 0.01016328
2025-05-29 00:21:24,082 - root - INFO -   Epoch 103/150	 Time: 7.962	 Loss: 0.00990808
2025-05-29 00:21:32,068 - root - INFO -   Epoch 104/150	 Time: 7.985	 Loss: 0.01027074
2025-05-29 00:21:40,032 - root - INFO -   Epoch 105/150	 Time: 7.964	 Loss: 0.00961461
2025-05-29 00:21:48,090 - root - INFO -   Epoch 106/150	 Time: 8.057	 Loss: 0.00988190
2025-05-29 00:21:56,072 - root - INFO -   Epoch 107/150	 Time: 7.981	 Loss: 0.00948996
2025-05-29 00:22:04,047 - root - INFO -   Epoch 108/150	 Time: 7.975	 Loss: 0.01019392
2025-05-29 00:22:12,005 - root - INFO -   Epoch 109/150	 Time: 7.958	 Loss: 0.00984488
2025-05-29 00:22:19,960 - root - INFO -   Epoch 110/150	 Time: 7.955	 Loss: 0.00955481
2025-05-29 00:22:27,909 - root - INFO -   Epoch 111/150	 Time: 7.948	 Loss: 0.00945318
2025-05-29 00:22:35,901 - root - INFO -   Epoch 112/150	 Time: 7.992	 Loss: 0.00950085
2025-05-29 00:22:43,874 - root - INFO -   Epoch 113/150	 Time: 7.972	 Loss: 0.00962541
2025-05-29 00:22:51,936 - root - INFO -   Epoch 114/150	 Time: 8.062	 Loss: 0.00948556
2025-05-29 00:22:59,883 - root - INFO -   Epoch 115/150	 Time: 7.946	 Loss: 0.00960219
2025-05-29 00:23:07,835 - root - INFO -   Epoch 116/150	 Time: 7.952	 Loss: 0.00915108
2025-05-29 00:23:15,770 - root - INFO -   Epoch 117/150	 Time: 7.936	 Loss: 0.00931654
2025-05-29 00:23:23,711 - root - INFO -   Epoch 118/150	 Time: 7.940	 Loss: 0.00898852
2025-05-29 00:23:31,642 - root - INFO -   Epoch 119/150	 Time: 7.931	 Loss: 0.00896596
2025-05-29 00:23:39,619 - root - INFO -   Epoch 120/150	 Time: 7.977	 Loss: 0.00908355
2025-05-29 00:23:47,618 - root - INFO -   Epoch 121/150	 Time: 7.999	 Loss: 0.00868237
2025-05-29 00:23:55,693 - root - INFO -   Epoch 122/150	 Time: 8.074	 Loss: 0.00871480
2025-05-29 00:24:03,655 - root - INFO -   Epoch 123/150	 Time: 7.962	 Loss: 0.00884472
2025-05-29 00:24:11,618 - root - INFO -   Epoch 124/150	 Time: 7.963	 Loss: 0.00884128
2025-05-29 00:24:19,576 - root - INFO -   Epoch 125/150	 Time: 7.958	 Loss: 0.00870979
2025-05-29 00:24:27,545 - root - INFO -   Epoch 126/150	 Time: 7.969	 Loss: 0.00868203
2025-05-29 00:24:35,538 - root - INFO -   Epoch 127/150	 Time: 7.992	 Loss: 0.00883532
2025-05-29 00:24:43,502 - root - INFO -   Epoch 128/150	 Time: 7.964	 Loss: 0.00853838
2025-05-29 00:24:51,557 - root - INFO -   Epoch 129/150	 Time: 8.055	 Loss: 0.00869985
2025-05-29 00:24:59,591 - root - INFO -   Epoch 130/150	 Time: 8.034	 Loss: 0.00836442
2025-05-29 00:25:07,578 - root - INFO -   Epoch 131/150	 Time: 7.987	 Loss: 0.00835033
2025-05-29 00:25:15,536 - root - INFO -   Epoch 132/150	 Time: 7.957	 Loss: 0.00868619
2025-05-29 00:25:23,516 - root - INFO -   Epoch 133/150	 Time: 7.980	 Loss: 0.00849809
2025-05-29 00:25:31,457 - root - INFO -   Epoch 134/150	 Time: 7.941	 Loss: 0.00822818
2025-05-29 00:25:39,448 - root - INFO -   Epoch 135/150	 Time: 7.990	 Loss: 0.00864253
2025-05-29 00:25:47,412 - root - INFO -   Epoch 136/150	 Time: 7.964	 Loss: 0.00778247
2025-05-29 00:25:55,471 - root - INFO -   Epoch 137/150	 Time: 8.059	 Loss: 0.00841045
2025-05-29 00:26:03,459 - root - INFO -   Epoch 138/150	 Time: 7.988	 Loss: 0.00811927
2025-05-29 00:26:11,434 - root - INFO -   Epoch 139/150	 Time: 7.974	 Loss: 0.00807930
2025-05-29 00:26:19,397 - root - INFO -   Epoch 140/150	 Time: 7.962	 Loss: 0.00822811
2025-05-29 00:26:27,355 - root - INFO -   Epoch 141/150	 Time: 7.958	 Loss: 0.00819957
2025-05-29 00:26:35,298 - root - INFO -   Epoch 142/150	 Time: 7.943	 Loss: 0.00788404
2025-05-29 00:26:43,278 - root - INFO -   Epoch 143/150	 Time: 7.979	 Loss: 0.00755067
2025-05-29 00:26:51,234 - root - INFO -   Epoch 144/150	 Time: 7.956	 Loss: 0.00808258
2025-05-29 00:26:59,286 - root - INFO -   Epoch 145/150	 Time: 8.051	 Loss: 0.00827729
2025-05-29 00:27:07,232 - root - INFO -   Epoch 146/150	 Time: 7.946	 Loss: 0.00844303
2025-05-29 00:27:15,172 - root - INFO -   Epoch 147/150	 Time: 7.940	 Loss: 0.00780937
2025-05-29 00:27:23,151 - root - INFO -   Epoch 148/150	 Time: 7.978	 Loss: 0.00804711
2025-05-29 00:27:31,102 - root - INFO -   Epoch 149/150	 Time: 7.951	 Loss: 0.00793366
2025-05-29 00:27:39,074 - root - INFO -   Epoch 150/150	 Time: 7.972	 Loss: 0.00756666
2025-05-29 00:27:39,074 - root - INFO - Training time: 1196.027
2025-05-29 00:27:39,074 - root - INFO - Finished training.
2025-05-29 00:27:39,075 - root - INFO - Starting testing...
2025-05-29 00:27:41,406 - root - INFO - Testing time: 2.332
2025-05-29 00:27:41,418 - root - INFO - Test set AUC: 100.00%
2025-05-29 00:27:41,419 - root - INFO - Testing TPR95: 1.0000
2025-05-29 00:27:41,419 - root - INFO - Finished testing.
